# 引言

应需要，考虑搭建机器人的语音文本系统。旨在人通过语音输入给机器人，机器人能通过语音给人以反馈。我们需要完成大致的整个系统，完成上述目的，机器人的模型知识可以首先基于通用的数据集和训练的模型，之后有机会再进行特定场景的适应和微调。

  

# 大致方法和原理

大致方法如图，虚线右边是机器人的部分，左边是人类可见的部分，或者说输入、输出。

暂时无法在飞书文档外展示此内容

## Speech to text model / Automatic Speech Recognition（STT / ASR）

### 1. 输入信号预处理

首先，将原始语音信号进行预处理（模拟信号转数字信号）和特征提取。输入语音信号是一个一维时间序列。

#### 特征提取

常用的特征提取方法包括短时傅里叶变换（STFT）和**梅尔频谱图（Mel-spectrogram）**。

### 2. 特征输入到神经网络（Encoder）

将提取的特征输入到神经网络模型中。假设神经网络模型包括卷积神经网络（CNN）、循环神经网络（RNN）、连接主义时序分类（CTC）层或Transformer模型。

### 3. 输出解码（Decoder）

对于输出解码，可以使用连接主义时序分类（CTC）或序列到序列（Seq2Seq）模型。

### 4. 损失函数和优化

对于 Seq2Seq 模型，通常使用交叉熵损失函数。

  

## Text to speech model（TTS）

### 1. 输入文本处理

首先，将输入的文本序列转换为嵌入表示。通常，输入文本是字符序列或词语序列，通过词嵌入（embedding）层将其转换为嵌入向量。

### 2. 编码器（Encoder）

编码器将嵌入向量序列转换为隐藏表示。编码器可以是循环神经网络（RNN）或 Transformer。

### 3. 解码器（Decoder）

解码器根据编码器的隐藏表示生成频谱图（如梅尔频谱图）。解码器可以是 RNN 或 Transformer。

### 4. 频谱图生成

将解码器的输出转换为频谱图。生成**梅尔频谱图（Mel-spectrogram）。**

### 5. 波形重建

通过频谱图生成波形，常用的模型包括 WaveNet、WaveGlow 或 Griffin-Lim 算法。

### 6. 损失函数

常用的损失函数包括均方误差（MSE）损失和熵损失。在使用 WaveNet 进行波形生成时，通常使用交叉熵损失。

  

  

还有一种方法是某个软件系统能够做到字素和音素的对应，除了一一对应外我们还需要建立上下文关系进行结果的矫正。

  

## Large Language Model

主流的GPT式生成式大型语言模型，包括LLaMA3、ChatGLM、GPT等。

Text-to-text 系统是指将输入文本转换为另一种形式的输出文本的系统。

  

  

  

# 整体系统框架

假设场景：一个人和一个机器人的对话。

启动和管理依赖：

1. 使用微服务架构。在一个操作系统上启动多个容器，隔离相应的依赖，每个容器包含各自的项目，暂时分TTS，ASR，LLM三或者两个容器（TTS、ASR为一个容器），每个容器启动各自服务，用主服务来调度各自服务的通信，最后得到需要的输出
    

  

技术栈：docker/kubernetes, python, flask, pytorch/hugging face...

相关项目：

https://github.com/PaddlePaddle/PaddleSpeech

https://github.com/openai/whisper

https://github.com/kaldi-asr/kaldi

  

# 实践

- 参考社区的主要开源项目。包括hugging face、github、Meta/Google/百度/字节/阿里/腾讯/清华/讯飞等的项目
    

![](https://cpgkpm53xz.feishu.cn/space/api/box/stream/download/asynccode/?code=NDEyZTRlNWY4NDViNzdiZDU4YTk4OWIxNDJmOGRlZTBfejZBTjJMUk9ScW55QzNwbDlZdUVjRVBmMVVxQkY4VnRfVG9rZW46VXJQRWJQb1JUb3B6U0F4aEJhbmNmb1lTbjdnXzE3MjAxNjc1NjU6MTcyMDE3MTE2NV9WNA)

- 读一些语音和本文模型相关综述，了解必要的理论和细节，参考商业公司的论文和仓库。
    
- 根据手上的资源，上手成熟的方案。
    
- 进一步定制化和优化。可能需要监督训练和微调。
    
- 部署在机器人上。
    
- 方法中的model可以是通过API访问得到模型输出，已知有很多大语言模型的API，也有TTS模型的API。
    

# 注意点

- audio和speech的区别，音频是一个广义的概念，包含所有类型的声音信号，而语音是音频的一部分，特指人类的语言声音。两者在内容、应用和处理技术上有显著区别。
    
- Text to text系统是指将输入文本转换为另一种形式的输出文本的系统。它们通常用于自然语言处理（NLP）任务，如翻译、摘要、文本生成等。Text-to-text 系统通常针对特定任务，而 LLM 是一个通用的语言模型，能够执行多种自然语言处理任务。
    
- 关于TTS和STT是否可以理解为就是一个逆过程，像FFT和iFFT一样，我找了两个文章有相关的讨论，见引用第三点、第四点。
    
- TTS和STT是否都是单一的encoder和encoder的范式？好像不是，根据论文https://api.semanticscholar.org/CorpusID:259203049
    
- 按照上述大致方法，我们需要三个不同的模型完成我们的任务。是否存在多模态大模型能直接处理speech，给出一个很好的反馈？
    
- speech和text各类任务中要确定其中文能力，在许多开源的项目和数据集中，可能是多语言或者只有英文的，我们要在语言种类做限定和定制化。
    
- 中文语音上含纳各类方言，文字上也较为复杂，使用机器人的场景也可能包含噪音的情况，以及发声者说话不连续等情况。
    
- 整个多个数据集进行训练也是一个挑战
    

# 引用

> https://huggingface.co/
> 
> https://murf.ai/resources/text-to-speech-vs-speech-to-text/
> 
> https://ai.meta.com/blog/multilingual-model-speech-recognition/
> 
> https://www.linkedin.com/pulse/text-speech-vs-whats-difference-oriserve-805ec/
> 
> https://cloud.baidu.com/article/3176025
> 
> https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard
> 
> gpt4o的问答
> 
> https://api.semanticscholar.org/CorpusID:259203049