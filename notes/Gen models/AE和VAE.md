自编码器（Autoencoder, AE）和变分自编码器（Variational Autoencoder, VAE）是两类非常重要的神经网络模型，它们都被用于学习数据的低维表示或者隐变量表示，但其背后有不同的数学框架和目标函数。我们将分别介绍自编码器和变分自编码器的数学原理，并重点比较它们的异同。

### 1. 自编码器（Autoencoder, AE）

自编码器是一种无监督学习模型，主要用于学习一种低维的、紧凑的特征表示。其结构通常包括一个编码器和一个解码器：

- **编码器**：将输入$x$压缩到一个低维的隐变量表示$z$中。编码器通常被表示为一个参数化函数$f_\theta(x)$，其中$\theta$是神经网络的参数。即：  
 $$
  z = f_\theta(x),
 $$
 $z$通常比输入$x$维度小，表示编码后的隐变量。

- **解码器**：将隐变量$z$解码回原来的输入空间，用于尽可能还原输入$x$。解码器也被表示为一个参数化函数$g_\phi(z)$，其中$\phi$是解码器的参数。即：
 $$
  \hat{x} = g_\phi(z),
 $$
  其中$\hat{x}$是解码后的重建数据。

自编码器的目标是最小化输入$x$与重建$\hat{x}$之间的差异，这通常通过最小化重建损失函数来实现，例如均方误差 (MSE)：

$$
L_{\text{AE}}(\theta, \phi) = \| x - \hat{x} \|^2 = \| x - g_\phi(f_\theta(x)) \|^2.
$$

通过这个过程，自编码器学习到了数据的低维表示$z$，这可以用于数据压缩、去噪等任务。

#### 数学总结：
- **编码**：$z = f_\theta(x)$
- **解码**：$\hat{x} = g_\phi(z)$
- **目标函数**：最小化$\| x - \hat{x} \|^2$

### 2. 变分自编码器（Variational Autoencoder, VAE）

变分自编码器是一种生成模型，与普通自编码器不同，VAE 不仅学习一个单一的隐变量表示$z$，而是学习隐变量的**概率分布**。它基于贝叶斯推断框架，通过优化证据下界 (ELBO) 来逼近数据分布。

#### 2.1 变分推断
VAE 试图学习潜在变量$z$与数据$x$之间的联合分布$p(x, z)$，其中$p(x)$表示数据的真实分布，$p(z)$是隐变量的先验分布（通常假设为高斯分布$\mathcal{N}(0, I)$）。生成模型可以表示为：
$$
p(x) = \int p(x|z) p(z) \, dz.
$$

然而，由于积分的计算在高维空间中非常困难，VAE 使用变分推断来近似后验分布$p(z|x)$。变分自编码器假设后验分布$p(z|x)$可以用一个简单的分布$q(z|x)$来近似，这个$q(z|x)$通常也是一个高斯分布，且其均值和方差由编码器网络输出。

#### 2.2 证据下界 (ELBO)
为了优化模型，VAE 最大化数据的对数似然$\log p(x)$，但由于这个目标很难直接计算，VAE 转而最大化证据下界 (ELBO)。ELBO 是对数据对数似然的下界，公式如下：

$$
\log p(x) \geq \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{\text{KL}}(q(z|x) \| p(z)),
$$
其中：
-$\mathbb{E}_{q(z|x)}[\log p(x|z)]$是重建项，表示根据隐变量$z$生成数据$x$的似然（即重建误差）。
-$D_{\text{KL}}(q(z|x) \| p(z))$是 KL 散度，衡量后验分布$q(z|x)$与先验分布$p(z)$之间的差异。

VAE 通过优化 ELBO 来同时逼近后验分布$q(z|x)$和最大化数据的重建可能性。

#### 2.3 编码器和解码器的结构
VAE 的编码器和解码器结构类似于普通的自编码器，但编码器输出的是隐变量分布的参数（均值和方差），而不是确定性的隐变量。具体来说：

- **编码器**：输出隐变量的均值$\mu_\theta(x)$和方差$\sigma^2_\theta(x)$，即$z \sim q_\theta(z|x) = \mathcal{N}(\mu_\theta(x), \sigma^2_\theta(x))$。
- **解码器**：输入隐变量$z$，输出重建的$\hat{x}$，即$p(x|z)$。

为了进行反向传播，VAE 使用了重参数化技巧，将隐变量的采样过程重参数化为可微分的形式（详见前述的重参数化技巧）。

#### 数学总结：
- **编码器**：学习隐变量的分布$q_\theta(z|x)$，输出均值$\mu_\theta(x)$和方差$\sigma_\theta(x)$。
- **解码器**：根据隐变量$z$来生成数据$\hat{x}$。
- **目标函数**：最大化 ELBO，即同时最小化重建误差和 KL 散度。

### 3. VAE 与 AE 的比较

| 特性                   | 自编码器 (AE)                    | 变分自编码器 (VAE)                |
|------------------------|----------------------------------|-----------------------------------|
| **目的**               | 学习确定的低维隐变量表示$z$    | 学习隐变量的概率分布$q(z|x)$|
| **隐变量表示**         | 单一的隐变量$z$               | 隐变量的分布$q(z|x)$          |
| **损失函数**           | 重建误差 (如 MSE)                | ELBO: 重建误差 + KL 散度         |
| **生成模型**           | 不是生成模型，主要用于数据压缩      | 是生成模型，可以生成新数据        |
| **训练过程**           | 直接最小化重建损失                | 最大化 ELBO（涉及 KL 散度）       |

### 4. 总结

- **AE** 是通过一个编码器和解码器网络直接学习输入数据的紧凑表示，并以重建损失为目标函数，主要用于数据压缩和去噪。
- **VAE** 是一种生成模型，基于概率图模型和变分推断，学习数据的潜在分布，能够生成新数据。它通过优化 ELBO 来最大化数据对数似然，并通过 KL 散度约束隐变量的分布接近先验。

