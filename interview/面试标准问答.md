---
tags:
  - note
---
### 语言语法杂项

#### C++

- 虚函数，虚函数表，虚指针，动态绑定
虚函数，基类带`virtual`，纯虚函数末尾加`=0`

![[Pasted image 20231013224414.png]]
```C++
int main() 
{    
	Base baseObj;     
	Derived derivedObj;      
	
	// 使用基类指针     
	Base* ptr;      
	
	// 动态绑定：根据对象类型调用正确的虚函数     
	ptr = &baseObj;     
	ptr->show(); // 输出 "Base::show()"      
	
	ptr = &derivedObj;     
	ptr->show(); // 输出 "Derived::show()"      
	return 0; 
}

```


- C++的智能指针是一种强大的工具，用于管理动态分配的内存，以帮助避免内存泄漏和悬挂指针等问题。C++11引入了智能指针的概念，主要包括以下两种类型：`std::shared_ptr`和`std::unique_ptr`，以及C++17引入的`std::weak_ptr`。

1. **std::shared_ptr：**
   - `std::shared_ptr`是一个智能指针，它允许多个指针共享同一个动态分配的对象。它使用引用计数来跟踪有多少个`std::shared_ptr`共享同一个对象。只有当最后一个`std::shared_ptr`销毁时，对象才会被释放。这使得多个指针可以安全地访问和管理相同的资源。

   示例：
   ```cpp
   std::shared_ptr<int> sharedPtr1 = std::make_shared<int>(42);
   std::shared_ptr<int> sharedPtr2 = sharedPtr1; // 共享同一个int对象
   ```

2. **std::unique_ptr：**
   - `std::unique_ptr`是一个智能指针，它表示独占所有权的指针，即只有一个`std::unique_ptr`可以拥有和管理一个对象。当`std::unique_ptr`被销毁时，它会自动释放所管理的对象。这确保了在同一时间只有一个指针可以操作该对象，从而避免了悬挂指针和资源泄漏。

   示例：
   ```cpp
   std::unique_ptr<int> uniquePtr1 = std::make_unique<int>(42);
   // std::unique_ptr<int> uniquePtr2 = uniquePtr1; // 错误，不允许复制
   ```

3. **std::weak_ptr：**
   - `std::weak_ptr`是用于协助`std::shared_ptr`的一种辅助智能指针。它不增加引用计数，但可以从一个`std::shared_ptr`构造而来，用于检查所管理的对象是否存在。它通常用于解决`std::shared_ptr`的循环引用问题，防止内存泄漏。（**循环引用**发生在多个对象之间互相持有彼此的智能指针，从而导致它们的引用计数永远不会降为零，因此这些对象永远不会被销毁，即使它们不再被程序使用。）

   示例：
   ```cpp
   std::shared_ptr<int> sharedPtr = std::make_shared<int>(42);
   std::weak_ptr<int> weakPtr = sharedPtr;

   // 使用weakPtr检查对象是否存在
   if (std::shared_ptr<int> sharedPtr2 = weakPtr.lock()) {
       // 对象存在，可以使用sharedPtr2
   } else {
       // 对象已经被释放
   }
   ```

智能指针的使用有助于简化内存管理，减少手动释放内存的错误，提高代码的可维护性和可读性。在使用智能指针时，记住选择合适的智能指针类型，避免循环引用，以确保正确地管理内存。

- 在C++11及之后的标准中，推荐使用`nullptr`来代替以前的`NULL`或`0`，主要有以下原因：

1. **类型安全**：`nullptr`是一个特殊的空指针常量，它具有自己的类型`nullptr_t`。这意味着编译器可以检测到使用`nullptr`的错误，因为它不能随意地与其他类型的指针进行混合使用。
    
2. **明确意图**：使用`nullptr`能够清晰地表达代码的意图，即将一个指针初始化为"空"，而不是混淆或误解，因为`0`在C++中既可以表示空指针，也可以表示整数值。
    
3. **避免重载冲突**：使用`0`或`NULL`时，存在与整数类型的重载冲突的潜在风险。一些函数可能会以整数类型为参数重载，这可能导致不正确的函数匹配。
    
4. **与新标准兼容**：C++11引入了`nullptr`，并将其作为新标准的一部分。因此，使用`nullptr`可以使代码更具现代化，并与新标准保持兼容。
    
5. **更严格的编译器检查**：现代编译器通常会警告或报错，如果发现了潜在的指针问题，使用`nullptr`可以帮助编译器更好地检测到这些问题。

在C++中，`::` 是作用域解析运算符（Scope Resolution Operator）的符号。它有以下几种主要用途：

1. **命名空间限定**：用于区分相同名称的标识符位于不同的命名空间。例如，`namespace1::variable` 表示变量在 `namespace1` 命名空间中。
    
2. **类的静态成员访问**：用于访问类的静态成员。例如，`MyClass::staticMember` 表示访问 `MyClass` 类的静态成员。
    
3. **全局变量访问**：用于访问全局命名空间中的变量。例如，`::globalVar` 表示访问全局变量 `globalVar`。
    
4. **基类成员访问**：用于访问基类中的成员，特别在派生类中。例如，`BaseClass::member` 表示访问基类 `BaseClass` 中的成员。
    
5. **解析函数重载**：在函数重载时，`::` 可以用于指定特定版本的函数。例如，`myFunction(int)` 和 `myFunction(double)` 可以通过 `myFunction::(int)` 和 `myFunction::(double)` 来区分。
    
6. **访问类的内部类型**：用于访问类的内部类型（嵌套类型）。例如，`MyClass::NestedType` 表示访问 `MyClass` 类中的嵌套类型。
    

`::` 的具体含义和用法取决于上下文，但总的来说，它用于指定标识符所属的作用域，以解决名称冲突和限定标识符的范围。

##### C++多线程/多进程

`Mutex`（互斥锁），`Semaphore`（信号量），和 `Lock`（锁）都是用于多线程或多进程编程中实现同步的机制，但它们在实现和使用上有一些区别：

1. **Mutex（互斥锁）:**
   - **性质：** 互斥锁是一种用于保护共享资源的同步机制，确保在任何时刻只有一个线程（或进程）能够访问被保护的资源。
   - **实现：** 通常是通过在代码中插入临界区域，只允许一个线程访问这个区域，其他线程必须等待。
   - **使用：** 适用于需要独占访问共享资源的情况，例如访问共享变量。

2. **Semaphore（信号量）:**
   - **性质：** 信号量是一种更为通用的同步原语，可以控制多个线程同时访问共享资源的数量。
   - **实现：** 信号量内部有一个计数器，每个 `acquire` 操作减少计数器，每个 `release` 操作增加计数器。当计数器为零时，后续的 `acquire` 操作将被阻塞，直到有线程执行 `release` 操作。
   - **使用：** 适用于限制资源的数量，例如连接池的大小，控制同时访问某资源的线程或进程数量。

3. **Lock（锁）:**

1. **互斥锁（`std::mutex`）:**

```cpp
#include <iostream>
#include <thread>
#include <mutex>

std::mutex myMutex;

void sharedResourceAccess(int id) {
    std::lock_guard<std::mutex> lock(myMutex);
    std::cout << "Thread " << id << " is accessing the shared resource." << std::endl;
    // 在这里进行对共享资源的操作
}

int main() {
    const int numThreads = 3;
    std::thread threads[numThreads];

    for (int i = 0; i < numThreads; ++i) {
        threads[i] = std::thread(sharedResourceAccess, i + 1);
    }

    for (int i = 0; i < numThreads; ++i) {
        threads[i].join();
    }

    return 0;
}
```

2. **信号量（`std::semaphore`）:**

C++20 开始支持 `std::semaphore`，以下是一个简单的使用示例：

```cpp
#include <iostream>
#include <thread>
#include <semaphore>

std::binary_semaphore mySemaphore(1);  // 初始计数为1，实现互斥锁的效果

void sharedResourceAccess(int id) {
    mySemaphore.acquire();  // 获取信号量，类似于互斥锁的 lock 操作
    std::cout << "Thread " << id << " is accessing the shared resource." << std::endl;
    // 在这里进行对共享资源的操作
    mySemaphore.release();  // 释放信号量，类似于互斥锁的 unlock 操作
}

int main() {
    const int numThreads = 3;
    std::thread threads[numThreads];

    for (int i = 0; i < numThreads; ++i) {
        threads[i] = std::thread(sharedResourceAccess, i + 1);
    }

    for (int i = 0; i < numThreads; ++i) {
        threads[i].join();
    }

    return 0;
}
```

3. **锁（`std::lock_guard`）:**

```cpp
#include <iostream>
#include <thread>
#include <mutex>

std::mutex myMutex;

void sharedResourceAccess(int id) {
    std::lock_guard<std::mutex> lock(myMutex);
    std::cout << "Thread " << id << " is accessing the shared resource." << std::endl;
    // 在这里进行对共享资源的操作
}

int main() {
    const int numThreads = 3;
    std::thread threads[numThreads];

    for (int i = 0; i < numThreads; ++i) {
        threads[i] = std::thread(sharedResourceAccess, i + 1);
    }

    for (int i = 0; i < numThreads; ++i) {
        threads[i].join();
    }

    return 0;
}
```

这些示例演示了如何使用C++中的互斥锁、信号量（C++20 起支持），以及锁来保护共享资源，确保在任何时刻只有一个线程能够访问。选择使用哪种同步机制通常取决于具体的需求和设计。

#### Python

- 迭代器和生成器
在Python中，迭代器（Iterator）和生成器（Generator）都用于支持迭代（循环）操作，使用==for each语句使用==。但它们在实现和使用方式上有一些重要的区别。让我们详细了解它们：

**迭代器（Iterator）**：

1. **定义和工作原理**：
    - 迭代器是一个具有`__iter__()`和`__next__()`方法的对象，可以用于逐个访问可迭代对象的元素。
    - `__iter__()`方法返回迭代器对象自身，而`__next__()`方法返回下一个元素。当没有元素可迭代时，`__next__()`方法引发`StopIteration`异常。
2. **使用方式**：
    - 迭代器通常需要手动编写，它们是一种显式的迭代机制。
    - 使用`iter()`函数创建一个迭代器，并使用`next()`函数获取下一个元素。
3. **适用场景**：
    - 适用于需要自定义迭代逻辑的情况，如遍历文件行、数据库查询结果等。

**生成器（Generator）**：

1. **定义和工作原理**：
    
    - 生成器是一种特殊的迭代器，它使用函数和`yield`语句来生成值。当函数包含`yield`语句时，它成为一个生成器函数。
    - 调用生成器函数不会执行函数体，而是返回一个生成器对象。生成器对象可以用于逐个生成值，并在`yield`处暂停执行。
2. **使用方式**：
    - 生成器更简单，无需手动实现`__iter__()`和`__next__()`方法。
    - 可以使用生成器表达式创建简单的生成器，或者编写生成器函数。
4. **适用场景**：
    - 适用于需要按需生成数据的情况，如处理大数据集合、无限序列、延迟计算等。

**区别**：

1. **实现**：迭代器需要显式编写`__iter__()`和`__next__()`方法，而生成器使用函数和`yield`语句自动生成。
    
2. **内存消耗**：生成器通常具有更低的内存消耗，因为它们以惰性方式生成数据，而迭代器可能需要提前生成并存储整个数据集合。
    
3. **使用方式**：生成器更容易使用，因为它们不需要手动处理`StopIteration`异常和循环逻辑。
    
4. **迭代次数**：生成器通常用于处理无限序列或需要大量迭代的情况，而迭代器通常用于处理有限数据集合。
    

综上所述，生成器是一种更高级和更方便的迭代方式，特别适用于需要处理大数据集合或延迟计算的情况。但在某些情况下，自定义迭代器可能更灵活，因为它们允许您完全控制迭代逻辑。

- python格式化字符串
```python
{M:=.Nf}
# M宽度 : f符号 . 浮点小数 N小数位后 
```

##### python多线程
`threading` 模块和 `multiprocessing` 模块是 Python 中用于实现并发编程的两个主要模块，它们分别提供了线程和进程的支持。下面是它们之间的一些主要区别：

1. **执行方式：**
   - **`threading`：** 提供了对线程的支持，线程是轻量级的执行单元，运行在同一进程的多个线程共享相同的内存空间。由于全局解释器锁（Global Interpreter Lock，GIL）的存在，==Python 的线程在某一时刻只允许一个线程执行 Python 字节码==，因此多线程并不能实现真正的并行执行。
   - **`multiprocessing`：** 提供了对进程的支持，进程是独立的执行单元，每个进程都有自己独立的内存空间。因为每个进程都有自己的解释器和 GIL，所以 `multiprocessing` 允许多个进程并行执行，适用于多核系统。

2. **内存管理：**
   - **`threading`：** 所有线程共享同一进程的内存空间，因此需要谨慎管理共享数据，以避免竞态条件（Race Condition）等问题。
   - **`multiprocessing`：** 每个进程有自己独立的内存空间，不共享全局解释器锁，因此数据共享较为简单。但是，由于进程之间不共享内存，数据传递需要使用进程间通信（Inter-Process Communication，IPC）的机制，如队列、管道、套接字、信号量等。

3. **通信机制：**
   - **`threading`：** 线程之间可以通过共享内存来进行通信，也可以使用锁等同步原语来确保数据一致性。
   - **`multiprocessing`：** 进程之间通信相对复杂，需要使用 IPC 机制，如 `multiprocessing.Queue`、`multiprocessing.Pipe` 等。
4. 同步机制：
   - 同步原语，LOCK(互斥锁)， RLOCK（递归锁）, semaphore, pool, event
   - `acquire`,`release` 相当于PV操作。本模块提供的所有具有 `acquire` 和 `release` 方法的对象都可用作 `with` 语句的上下文管理器。 进入语句块时将调用 `acquire` 方法，退出语句块时将调用 `release` 方法
1. **适用场景：**
   - **`threading`：** 适用于 I/O 密集型任务，如网络请求、文件操作等，因为线程可以在等待 I/O 操作的同时执行其他任务。
   - **`multiprocessing`：** 适用于 CPU 密集型任务，如计算密集型算法，因为进程可以在多个 CPU 核心上并行执行。

2. **开销：**
   - **`threading`：** 线程的创建和销毁开销较小，因为线程共享同一进程的资源。
   - **`multiprocessing`：** 进程的创建和销毁开销相对较大，因为每个进程都有自己独立的资源。

总体而言，`threading` 适用于并发编程中的轻量级任务和 I/O 密集型任务，而 `multiprocessing` 更适用于 CPU 密集型任务和需要并行处理的场景。选择使用哪种模块取决于任务的性质以及系统硬件的特点。


### 深度学习杂项

在所有情况下，池化有助于使表示对于输入的小翻译/平移近似不变。
In all cases, pooling helps to make the representation approximately invariant to small translations of the input.

稀疏相互作用、参数共享和等变表示，它们是卷积神经网络（CNN）的核心特性。

1. **稀疏相互作用（Sparse Interactions）：** 在传统的全连接神经网络中，每个神经元都与上一层的每个神经元相连接，这会导致大量的参数和计算复杂度。卷积层采用了稀疏相互作用，即每个神经元只与输入数据的一小部分区域相连接。这意味着神经元仅处理输入数据的局部信息，而不是整个输入。这种稀疏性减少了参数数量，减小了计算负担，同时保留了重要的特征。
    
2. **参数共享（Parameter Sharing）：** 在卷积层中，同一个卷积核（滤波器）被用于处理输入数据的不同位置。这意味着卷积核的权重在整个输入中共享。参数共享有两个主要好处：首先，它减少了模型的参数数量，因为每个卷积核只有一个集合的权重，而不是为每个位置都有一个独立的权重；其次，它导致了一种特征学习，即卷积核学习到的特征对于输入数据的不同位置是相同的。这使得卷积层对于平移不变性具有更好的表示能力，因为无论特征在图像的哪个位置出现，模型都可以识别它。
    
3. **等变表示（Equivariant Representation）：** 等变性是指当输入数据的变换（例如平移、旋转等）时，模型的输出也相应地发生变化。卷积层具有等变性，即它们能够保持一种特定的输入变换。例如，如果卷积核学习到了检测垂直边缘的特征，那么无论这些垂直边缘在图像的哪个位置，卷积层都能够检测到它们。这种性质有助于CNN学习到图像中的不同特征，并使得模型对于输入数据的变换具有一定的鲁棒性。
    

范数（Norm）是线性代数中用于衡量向量大小或长度的一个重要概念。它是一个将向量映射到实数的函数，通常表示为 ||x||，其中 x 是向量。

1. **L1 范数（曼哈顿范数）**：
   L1 范数是向量中所有元素绝对值之和。对于一个 n 维向量 x，L1 范数定义如下：
   $$||x||_1 = |x_1| + |x_2| + ... + |x_n|$$
   它测量了向量中每个元素到原点的距离总和，通常用于特征选择和稀疏性。

2. **L2 范数（欧几里得范数）**：
   L2 范数是向量中所有元素的平方和的平方根。对于一个 n 维向量 x，L2 范数定义如下：
   $$||x||_2 = \sqrt{x_1^2 + x_2^2 + ... + x_n^2}$$
   它测量了向量的长度或模，通常在机器学习中广泛使用，例如在支持向量机中的正则化。

3. **无穷范数（最大值范数）**：
   无穷范数是向量中绝对值最大的元素。对于一个 n 维向量 x，无穷范数定义如下：
   $$||x||_{\infty} = \max(|x_1|, |x_2|, ..., |x_n|)$$
   它测量了向量中最大的绝对值，通常用于优化问题和误差分析。



这是三个常用的回归问题中用于衡量模型性能的指标：

1. **均方误差 (MSE - Mean Squared Error)**：均方误差是最常见的回归性能度量。它计算观测值与模型预测值之间的差的平方的均值。MSE值越小表示模型对数据拟合得越好。

   公式为：
   
   $$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

   其中，$n$是数据点的数量，$y_i$是真实观测值，$\hat{y}_i$是模型的预测值。

2. **平均绝对误差 (MAE - Mean Absolute Error)**：平均绝对误差是观测值与模型预测值之间的绝对差的均值。MAE对异常值不敏感，因为它不关心误差的平方，而只是绝对值。

   公式为：

   $$MAE = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$

3. **平均绝对百分比误差 (MAPE - Mean Absolute Percentage Error)**：MAPE是一个百分比误差度量，它计算观测值与模型预测值之间的绝对百分比误差的均值。

   公式为：

   $$MAPE = \frac{1}{n}\sum_{i=1}^{n}\left|\frac{y_i - \hat{y}_i}{y_i}\right| \times 100\%$$

   这个指标通常用于表示模型误差相对于观测值的百分比。 MAPE值越低表示模型的拟合度越好。

这些指标用于评估回归模型的性能，你可以根据具体的问题选择合适的指标。 MSE通常用于一般性能度量，MAE对异常值不敏感，而MAPE用于百分比误差度量。

混淆矩阵：
![[Pasted image 20231204133212.png]]

| 预测为 \实际为 | Positive | Negative |
| :------: | :------: | :------: |
|   预测为阳   |    TP    |    FP    |
|   预测为阴   |    FN    |    TN    |

TF为预测相对真值为真还是假，PN是数据原本的GT
- **True Positive (TP)**：模型正确地将样本预测为当前类别。（预测对了，预测成正）
- **False Positive (FP)**：模型错误地将样本预测为当前类别，实际上样本属于其他类别。
- **True Negative (TN)**：模型正确地将样本预测为非当前类别。
- **False Negative (FN)**：模型错误地将样本预测为非当前类别，实际上样本属于当前类别。

- 召回率 准确率 F1 score
召回率（Recall）和准确率（Precision）是用于评估分类模型性能的两个重要指标，特别在二分类问题中经常使用。

0. 准确率（Accuracy）
$$ Accuracy = \frac{TP + TN}{TP + TN+FP+FN}$$

1. **召回率（Recall）：** 召回率是指模型识别出的正确正例数量占总正例数量的比例。它衡量了模型在所有实际正例中有多少被正确识别出来，也可以称为真正例率（True Positive Rate）。召回率的计算公式如下：

  $$ Recall = \frac{TP}{TP + FN}$$

   其中：
   - TP（True Positives）是被模型正确识别为正例的数量。
   - FN（False Negatives）是实际为正例但被模型错误识别为负例的数量。

   召回率的取值范围是0到1，它越接近1表示模型对正例的识别能力越强。

2. **精确率（Precision）：** 准确率是指模型识别出的正确正例数量占所有被模型识别为正例的样本数量的比例。它衡量了模型在所有被识别为正例的样本中有多少是真正的正例，也可以称为阳性预测值（Positive Predictive Value）。准确率的计算公式如下：

   $$Precision = \frac{TP}{TP + FP}$$

   其中：
   - TP（True Positives）是被模型正确识别为正例的数量。
   - FP（False Positives）是实际为负例但被模型错误识别为正例的数量。

   准确率的取值范围也是0到1，它越接近1表示模型对正例的识别能力越强。

在某些应用中，我们可能更关心召回率，希望尽量**不错过任何正例**，即**降低假阴性率**。而在另一些应用中，我们可能更**关心准确率**，希望确保模型的**预测结果是高度可信的**，即降低假阳性率。根据具体的应用场景，我们可以选择更适合的指标来评估模型性能。在一些情况下，我们也会使用综合指标，如F1分数，来综合考虑召回率和准确率。

**F1分数**（F1 Score）是一个用于评估分类模型性能的综合指标，它结合了召回率（Recall）和准确率（Precision），用于衡量模型在二分类问题中的表现。两者的==几何平均数==。

F1分数的计算公式如下：

$$F1 = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}$$



**ROC 曲线**：
- ROC 曲线是一个图形工具，用于可视化二分类模型在不同阈值下的性能表现。横轴表示假正例率（FPR，False Positive Rate FPR=FP/(FP+TN)），纵轴表示真正例率（TPR，True Positive Rate TPR=TP/(TP+FN)），即召回率。
- 模型在不同阈值下的预测结果被归为正类或负类，并根据不同阈值计算 FPR 和 TPR。ROC 曲线是 FPR 对 TPR 的关系图。
- 一个理想的模型的 ROC 曲线将尽可能靠近左上角，即 FPR 很低而 TPR 很高，这表示模型在各种情况下都表现良好。
- ROC 曲线下的面积 AUC 越接近1，表示模型性能越好；AUC 等于0.5时，表示模型的性能等同于随机猜测。

**AUC（Area Under the Curve）**：
- AUC 是 ROC 曲线下的面积，用于量化一个分类模型的性能。AUC 越大，模型的性能越好。
- AUC 的取值范围在0到1之间。一个完美的模型的 AUC 为1，而随机模型的 AUC 约为0.5。
- AUC 的直观解释是，在一个随机选择的正样本和负样本之间，模型将正样本排在前面的概率。

总结来说，ROC 曲线和 AUC 是用于评估分类模型性能的工具，特别适用于处理不平衡数据和比较多个模型的性能。如果你希望在两个或多个模型之间选择性能较好的模型，AUC 和 ROC 曲线是很有帮助的评估指标。


多分类混淆矩阵是用于评估多分类模型性能的一种表格，适用于三类或三类以上的分类问题。假设有K个类别，混淆矩阵的形式如下：

|        | 预测类别 1                   | 预测类别 2                   | ... | 预测类别 K                   |
| ------ | ------------------------ | ------------------------ | --- | ------------------------ |
| 实际类别 1 | 真正例 (True Positive, TP)  | 假负例 (False Negative, FN) | ... | 假负例 (False Negative, FN) |
| 实际类别 2 | 假正例 (False Positive, FP) | 真正例 (True Positive, TP)  | ... | 假负例 (False Negative, FN) |
| ...    | ...                      | ...                      | ... | ...                      |
| 实际类别 K | 假正例 (False Positive, FP) | 假负例 (False Negative, FN) | ... | 真正例 (True Positive, TP)  |


其中，每个单元格的含义如下：

- **真正例（True Positive, TP）：** 模型正确地预测为当前类别。
- **假负例（False Negative, FN）：** 模型错误地预测为其他类别。
- **假正例（False Positive, FP）：** 模型错误地预测为当前类别，但实际为其他类别。

通过多分类混淆矩阵，我们可以计算一系列评估模型性能的指标，包括准确率、精确率、召回率、F1 分数等。

在多分类问题中，这些指标的计算方式有所不同：

- **准确率（Accuracy）：** $\frac{\text{正确预测的样本数}}{\text{总样本数}}$
- **精确率（Precision）：** $\frac{\text{TP}}{\text{TP} + \text{FP}}$，其中 TP 是当前类别的真正例。
- **召回率（Recall）：** $\frac{\text{TP}}{\text{TP} + \text{FN}}$，其中 TP 是当前类别的真正例。
- **F1 分数：** $2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$

这些指标提供了对模型在多个类别上性能的全面评估。混淆矩阵的可视化和指标的计算可以帮助你更好地理解模型对每个类别的分类准确性和错误情况。

#### K means

K均值算法的数学描述如下：

假设有一个数据集 $X$ 包含 $n$ 个样本，每个样本表示为 $x_i$，其中 $i = 1, 2, ..., n$。算法的目标是将数据集划分成 $K$ 个簇，每个簇的中心表示为 $c_k$，其中 $k = 1, 2, ..., K$。

1. **选择K个初始中心点：**
   $$ C = \{c_1, c_2, ..., c_K\} $$

2. **分配数据点：**
   计算每个样本 $x_i$ 到每个中心点 $c_k$ 的距离，通常使用欧氏距离：
   $$ d(x_i, c_k) = \sqrt{\sum_{j=1}^{d}(x_{ij} - c_{kj})^2} $$
   其中，$d$ 是数据的维度。

   将样本 $x_i$ 分配到距离最近的中心点 $c_k$ 所属的簇。

3. **更新中心点：**
   对于每个簇 $k$，计算其所有样本的平均值，得到新的中心点：
   $$ c_k = \frac{1}{|C_k|}\sum_{x_i \in C_k}x_i $$
   其中，$C_k$ 表示簇 $k$ 中的所有样本。

4. **重复步骤2和步骤3：**
   重复分配数据点和更新中心点的过程，直到满足停止条件。

5. **输出结果：**
   当停止条件满足后，K均值算法将给出 $K$ 个簇，每个簇包含一组相似的数据点。

实现示例：
```python
import numpy as np
import matplotlib.pyplot as plt

def kmeans(X, k, max_iters=100, tol=1e-4):
    # 初始化中心点
    centers = X[np.random.choice(range(len(X)), k, replace=False)]
    
    for _ in range(max_iters):
        # 计算每个样本到中心点的距离
        distances = np.linalg.norm(X[:, np.newaxis] - centers, axis=2)
        
        # 分配样本到最近的中心点
        labels = np.argmin(distances, axis=1)
        
        # 计算新的中心点
        new_centers = np.array([X[labels == i].mean(axis=0) for i in range(k)])
        
        # 判断是否收敛
        if np.linalg.norm(new_centers - centers) < tol:
            break
        
        # 更新中心点
        centers = new_centers
    
    return labels, centers

# 生成一些示例数据
np.random.seed(42)
X = np.concatenate([np.random.normal(loc=i, scale=1, size=(100, 2)) for i in range(3)])

# 使用自己实现的K均值算法进行聚类
k = 3
labels, centers = kmeans(X, k)

# 绘制结果
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Cluster Centers')
plt.title('K-Means Clustering (Self-Implemented)')
plt.legend()
plt.show()

```

#### Metrics

##### DICE/IoU loss

![[Pasted image 20231204133212.png]]
多分类的Dice Loss可以通过对每个类别分别计算Dice Loss并求平均来实现。在这种情况下，Dice Loss的计算方式与二分类时有一些变化。假设有$C$个类别，每个类别对应一个预测概率图和一个实际标签，那么多分类Dice Loss的计算可以通过以下公式：

$\ Dice Loss = 1 - \frac{1}{C} \sum_{i=1}^{C} \frac{2 \times |X_i \cap Y_i|}{|X_i| + |Y_i|}$

其中，$X_i$表示第$i$个类别的预测概率图，$Y_i$表示第$i$个类别的实际标签。

==可见dice coefficient是等同**F1 score**，直观上dice coefficient是计算$X$与$Y$的相似性，本质上则同时隐含precision和recall两个指标。可见dice loss是直接优化**F1 score**。==

下面是一个简单的Python代码示例，演示如何计算多分类Dice Loss：

```python
import numpy as np

def calculate_iou(box1, box2):
    """
    计算两个边界框的IoU（Intersection over Union）

    参数:
    box1, box2: 边界框，每个边界框是一个包含 (x, y, width, height) 的元组

    返回:
    iou: IoU的值
    """

    # 提取边界框的坐标和尺寸
    x1, y1, w1, h1 = box1
    x2, y2, w2, h2 = box2

    # 计算交集的坐标和尺寸
    intersection_x = max(x1, x2)
    intersection_y = max(y1, y2)
    intersection_width = max(0, min(x1 + w1, x2 + w2) - intersection_x)
    intersection_height = max(0, min(y1 + h1, y2 + h2) - intersection_y)

    # 计算交集和并集的面积
    intersection_area = intersection_width * intersection_height
    union_area = w1 * h1 + w2 * h2 - intersection_area

    # 计算IoU
    iou = intersection_area / union_area if union_area > 0 else 0

    return iou


# 另一种写法

def iou():
	x1,y1,x2,y2 = box1 #box1的左上角坐标、右下角坐标
	x3,y3,x4,y4 = box2 #box1的左上角坐标、右下角坐标
	
	#计算交集的坐标
	x_inter1 = max(x1,x3) #union的左上角x
	y_inter1 = max(y1,y3) #union的左上角y
	x_inter2 = min(x2,x4) #union的右下角x
	y_inter2 = min(y2,y4) #union的右下角y
	
	# 计算交集部分面积，因为图像是像素点，所以计算图像的长度需要加一
	# 比如有两个像素点(0,0)、(1,0)，那么图像的长度是1-0+1=2，而不是1-0=1
	interArea = max(0,x_inter2-x_inter1+1)*max(0,y_inter2-y_inter1+1)
	
	# 分别计算两个box的面积
	area_box1 = (x2-x1+1)*(y2-y1+1)
	area_box2 = (x4-x3+1)*(y4-y3+1)
	
	#计算IOU，交集比并集，并集面积=两个矩形框面积和-交集面积
	iou = interArea/(area_box1+area_box2-interArea)



def dice_coefficient(y_true, y_pred, axis=(1, 2), smooth=1e-8):
    intersection = np.sum(y_true * y_pred, axis=axis)
    union = np.sum(y_true, axis=axis) + np.sum(y_pred, axis=axis)
    dice = (2.0 * intersection + smooth) / (union + smooth)
    return dice

def dice_loss(y_true, y_pred):
    num_classes = y_true.shape[-1]
    loss = 1 - np.mean([dice_coefficient(y_true[..., i], y_pred[..., i]) for i in range(num_classes)])
    return loss

# 示例
# 假设y_true和y_pred是多分类的预测概率图，每个类别有对应的通道
y_true = np.array([[[1, 0, 0],
                    [0, 1, 1],
                    [1, 0, 1]],
                   
                   [[0, 1, 0],
                    [1, 0, 0],
                    [0, 1, 0]]])

y_pred = np.array([[[0.9, 0.1, 0.2],
                    [0.3, 0.8, 0.7],
                    [0.8, 0.2, 0.9]],
                   
                   [[0.2, 0.7, 0.1],
                    [0.6, 0.3, 0.4],
                    [0.1, 0.9, 0.3]]])

loss = dice_loss(y_true, y_pred)
print(f"Multi-Class Dice Loss: {loss}")
```

在这个示例中，`y_true`和`y_pred`分别是实际标签和模型的预测概率图，每个类别都有相应的通道。

##### mAP

1. **计算AP：** 在Precision-Recall曲线下方的面积即为Average Precision。AP是对模型在单个类别上性能的度量。
    
2. **计算mAP：** 对所有类别的AP取平均得到mAP。
##### CE

交叉熵（Cross Entropy）是一种用于测量两个概率分布之间差异的指标。在深度学习中，通常用于衡量模型输出的概率分布与实际标签的分布之间的差异。下面是交叉熵损失在图像矩阵上的具体运算过程。

假设有一个图像分类任务，每个图像有K个类别。令 $p = (p_1, p_2, ..., p_K)$ 为模型的预测概率分布， $q = (q_1, q_2, ..., q_K)$ 为实际的独热编码标签（实际上为0或1，表示属于哪个类别）。

交叉熵损失的公式如下：

$H(p, q) = - \sum_{i=1}^{K} q_i \log(p_i)$

在图像矩阵上的运算过程可以描述为：

1. **将图像输入模型，得到预测概率分布 $p$。**
2. **将实际标签 $q$ 转换为概率分布。**
   - 对于独热编码标签，直接使用 $q_i$。
   - 对于整数编码标签，将其转换为独热编码形式。
3. **代入公式，计算每个类别的交叉熵。**
   - 对于第 $i$ 个类别：
     $\text{CE}_i = - q_i \log(p_i)$
4. **将所有类别的交叉熵求和，得到总体交叉熵。**
   $H(p, q) = \sum_{i=1}^{K} \text{CE}_i$

这个过程可以用代码来表示，具体实现会根据使用的深度学习框架有所不同。以下是一个用Python和NumPy表示的简单例子：

```python
import numpy as np

# 模拟模型预测概率分布和实际标签
predictions = np.array([0.2, 0.7, 0.1])
actual_labels = np.array([0, 1, 0])  # 独热编码标签

# 计算交叉熵,##reduce##成一个CE值
cross_entropy = -np.sum(actual_labels * np.log(predictions))
print("交叉熵损失:", cross_entropy)
```

请注意，实际训练深度学习模型时，通常使用深度学习框架中提供的交叉熵损失函数，而不是手动实现。这些框架会优化计算，并处理一些数值稳定性的问题。上述代码仅用于说明交叉熵的基本概念。

##### Focal loss

在类别不平衡问题中，模型可能会倾向于预测出现频率更高的类别，而对于出现频率较低的类别则效果较差。Focal Loss通过调整损失函数的权重，使得模型更加关注难以分类的样本，从而缓解了类别不平衡问题。

Focal Loss的定义如下：

$$ \text{FL}(p_t) = -(1 - p_t)^\gamma \log(p_t) $$

其中，$p_t$ 是模型对样本属于正类别的概率，$\gamma$ 是调整因子，通常取正值，用于调整难以分类样本的权重。当样本属于负类别时，$(1 - p_t)^\gamma$ 会增大，降低对容易分类的样本的关注度；而对于难以分类的样本，$(1 - p_t)^\gamma$ 会减小，增加对这些样本的关注度。

具体步骤如下：

1. **计算模型输出的概率 $p$ ：** 对于二分类任务，$p$ 是模型输出的概率；对于多分类任务，$p$ 是softmax激活后的概率分布。

2. **计算 $p_t$：** 对于二分类任务， $p_t = \max(p, 1-p)$ ；对于多分类任务，$p_t$ 是模型输出概率的对应类别的概率。

3. **计算Focal Loss：** 使用上述公式计算Focal Loss。

在实际应用中，通常将Focal Loss与交叉熵损失结合，形成总体的损失函数：

$$ \text{Total Loss} = (1 - \text{FL Weight}) \times \text{Cross Entropy Loss} + \text{FL Weight} \times \text{Focal Loss} $$

其中，$\text{FL Weight}$ 是一个超参数，用于平衡交叉熵损失和Focal Loss的重要性。

使用Focal Loss的主要优点是能够缓解类别不平衡问题，提高模型对难以分类样本的关注度。这在一些目标检测、图像分割等任务中特别有帮助。

**多分类问题的Focal Loss：**

对于多分类问题，设模型有K个类别，每个样本有K个类别的概率分布。每个样本的标签为 $y = (y_1, y_2, ..., y_K)$，其中 $y_i$ 表示样本属于第i个类别的概率。模型的预测为 $p = (p_1, p_2, ..., p_K)$。

多分类的Focal Loss定义如下：

$\text{FL}(p, y) = - \sum_{i=1}^{K} y_i \cdot (1 - p_i)^\gamma \cdot \log(p_i)$

其中，$gamma$ 是Focal Loss的调整参数，通常取正值。这个公式对每个类别计算了Focal Loss，然后将它们求和得到总体的损失。

**二分类问题的Focal Loss：**

在二分类问题中，Focal Loss的定义相对简单，考虑了正类别和负类别的情况：

$\text{FL}(p) = - (1 - p)^\gamma \cdot \log(p)$

这个公式仅计算了正类别的Focal Loss。






#### 优化器

1. **梯度下降（Gradient Descent）**：
   - 基本的优化算法，通过计算损失函数关于参数的梯度来更新参数。
   - 缺点是可能收敛速度慢，容易陷入局部最小值。
   
2. **随机梯度下降（Stochastic Gradient Descent，SGD）**：
   - SGD是梯度下降的变种，每次更新只使用一小批数据（随机选择的样本）。
   - 更快的收敛速度，但可能会引入噪声。

3. **动量梯度下降（Momentum）**：
   - 引入动量项，通过累积之前梯度的移动平均来更新参数。
   - 有助于加速收敛，减小梯度震荡。

4. **AdaGrad**：
   - 自适应学习率的方法，根据参数的历史梯度平方累积来调整学习率。
   - 适合处理稀疏数据，但可能导致学习率过于缩小。

5. **RMSprop**（Root Mean Square Propagation）：
   - 也是一种自适应学习率的算法，但与AdaGrad不同，它使用梯度平方的移动平均。
   - 有助于解决AdaGrad中学习率过度下降的问题。

6. **Adadelta**：
   - 另一种自适应学习率算法，类似于RMSprop，但不需要手动设置初始学习率。

7. **Adam**（Adaptive Moment Estimation）：
   - 结合了**动量**和**自适应学习率**的方法，广泛应用于深度学习。
   - 具有较好的性能，通常是默认的选择。

8. **Nadam**（Nesterov-accelerated Adaptive Moment Estimation）：
   - 是Adam的变种，结合了Nesterov动量，通常比Adam更快。

9. **L-BFGS（Limited-memory Broyden-Fletcher-Goldfarb-Shanno）**：
   - 基于拟牛顿法，适合小型数据集和参数较少的网络。

10. **SGD with Learning Rate Scheduling**：
    - 在训练过程中逐渐降低学习率，以平衡快速收敛和稳定性。

不同的优化器适用于不同的情况，通常需要根据具体问题和数据来选择。一些优化器可能需要调整超参数，如学习率，以达到最佳性能。 Adam通常是一个较好的默认选择，但在某些情况下，其他优化器也可能表现更好。Adamw是加weight decay的Adam版本，梯度更新时会对之前的梯度有一个综合的罚项。
1. **Adam (Adaptive Moment Estimation):**
   - **原理：** Adam结合了梯度的一阶矩估计（梯度的指数移动平均）和二阶矩估计（梯度平方的指数移动平均）。它具有自适应学习率，使得对于不同参数的梯度可以使用不同的学习率。以及考虑==动量==，及过去的梯度对现在的梯度更新有一个势能，惯性
   - **公式：** $m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$, $v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$, $\hat{\alpha}_t = \frac{\alpha \sqrt{1 - \beta_2^t}}{1 - \beta_1^t}$, $w_{t+1} = w_t - \hat{\alpha}_t \cdot \frac{m_t}{\sqrt{v_t} + \epsilon}$.
   - **备注：** $\beta_1$ 和 $\beta_2$ 是衰减因子，$\epsilon$ 是平滑项，$\alpha$ 是初始学习率。

2. **SGD (Stochastic Gradient Descent):**
   - **原理：** SGD是最基本的优化算法，它在每一步使用整个训练集中的一个随机样本来更新模型参数。随机性有助于逃离局部最小值，但可能导致参数更新的不稳定。
   - **公式：** $w_{t+1} = w_t - \eta \cdot g_t$, 其中 $g_t$ 是在时间步 $t$ 的梯度，$\eta$ 是学习率。

3. **RMSprop (Root Mean Square Propagation):**
   - **原理：** RMSprop也是一种自适应学习率的算法，它使用梯度平方的==指数移动平均==来调整学习率。相比于Adam，RMSprop没有使用一阶矩估计。
   - **公式：** $v_t = \beta \cdot v_{t-1} + (1 - \beta) \cdot g_t^2$, $w_{t+1} = w_t - \frac{\eta}{\sqrt{v_t} + \epsilon} \cdot g_t$, 其中 $\beta$ 是衰减因子，$\epsilon$ 是平滑项。

4. **AdaGrad (Adaptive Gradient Algorithm):**
   - **原理：** AdaGrad是一种自适应学习率的算法，它根据每个参数的历史梯度平方和调整学习率，对于频繁出现的梯度较小的参数，学习率会较大。
   - **公式：** $s_t = s_{t-1} + g_t^2$, $w_{t+1} = w_t - \frac{\eta}{\sqrt{s_t} + \epsilon} \cdot g_t$, 其中 $s_t$ 是历史梯度平方和。

#### 训练时

1. **不收敛问题**：
   - **原因**：不收敛可能是由于不合适的学习率、训练数据的分布问题、过拟合等原因引起的。
   - **解决方法**：
     - 调整学习率：尝试不同的学习率，通常从小到大逐步调整，找到适合的学习率。
     - 观察训练数据：检查训练数据是否存在异常值或分布不均匀的情况，可能需要数据预处理。
     - 正则化：使用正则化技巧（如L1、L2正则化）来减少过拟合。
     - 更复杂的模型：增加模型容量，例如增加神经网络的层数或神经元的数量。

2. **梯度消失和梯度爆炸**：
   - **梯度消失**：在深层网络中，梯度可能逐渐变小，导致底层权重更新非常缓慢，模型难以训练。
   - **梯度爆炸**：梯度可能在反向传播中迅速增加，导致权重的大幅度更新，破坏训练稳定性。
   - **判断方法**：通常可以通过观察损失函数的训练曲线，如果损失不断增大，可能存在梯度爆炸问题；如果损失在训练初期下降但后来停滞，可能存在梯度消失问题。
   - **解决方法**：
     - **梯度消失**：
       - 使用适当的激活函数：例如，用ReLU（Rectified Linear Unit）代替Sigmoid或Tanh，因为它们更不容易引起梯度消失。
       - Batch Normalization：通过规范化输入数据来加速训练，减轻梯度消失问题。
       - 使用Skip Connections：在深层网络中引入跳跃连接，如ResNet，有助于信息的传递。
     - **梯度爆炸**：
       - 梯度裁剪（Gradient Clipping）：限制梯度的大小，使其不超过一个特定的阈值。
       - 使用合适的权重初始化：Xavier初始化等可以帮助减少梯度爆炸的发生。
       - 正则化：L2正则化可以帮助控制权重的大小。

3. **常用激活函数**：
   - 常用的激活函数包括：
     - **ReLU**（Rectified Linear Unit）：f(x) = max(0, x)。用于解决梯度消失问题，是目前最常用的激活函数。
     - **Sigmoid**：f(x) = 1 / (1 + e^(-x))。常用于输出层的二分类问题。
     - **Tanh**（双曲正切）：f(x) = (e^(x) - e^(-x)) / (e^(x) + e^(-x))。类似Sigmoid，但输出范围在-1到1之间。
     - **Leaky ReLU**：f(x) = x（x > 0），f(x) = ax（x <= 0，a是小于1的常数）。用于解决ReLU的负值问题。
     - **Swish**：f(x) = x * sigmoid(x)。具有类似ReLU的性质，但在一些情况下表现更好。

    1. **Leaky ReLU的主要优点**：
    - Leaky ReLU对于负数输入保持了一个小的梯度，这有助于避免“死亡神经元”问题，其中ReLU在训练过程中可能会停止学习，因为负数输入对应的梯度一直为零。
    - Leaky ReLU通常比普通的ReLU更具鲁棒性，可以在一定程度上处理梯度消失问题。
	2. **普通的ReLU的优点**：
    - 普通的ReLU是一种非常简单的激活函数，计算速度快，且在实践中表现良好。
    - 普通的ReLU在一些情况下可能会更快地收敛，因为它不涉及额外的超参数（如Leaky ReLU中的$\alpha$）。

选择激活函数通常取决于具体任务和实验结果。ReLU是一个常见的起点，但可以根据问题的需要尝试其他激活函数，以获得更好的性能。

#### 梯度消失和梯度爆炸

**判断梯度消失**：

1. **观察损失曲线**：如果训练过程中的损失函数在一开始迅速下降，然后趋于平稳，可能是梯度消失的迹象。梯度消失导致权重更新很小，模型无法有效学习。

2. **观察梯度统计**：监测每一层的梯度值，如果梯度值很小（接近零），特别是在深层网络的较低层，可能是梯度消失的迹象。

3. **使用激活函数**：如果模型使用Sigmoid或Tanh等饱和激活函数，这些函数在输入远离零时梯度变得很小，容易导致梯度消失（图像上），Sigmoid和Tanh函数在深度神经网络中的训练效果较差，通常更适合于浅层网络或输出层的二元分类问题。改用ReLU或其他非饱和激活函数可能有助于缓解这个问题。

**判断梯度爆炸**：

1. **观察损失曲线**：如果训练过程中的损失函数迅速增加，甚至变为NaN，这可能是梯度爆炸的迹象。梯度爆炸导致权重更新非常大，模型不稳定。

2. **观察梯度统计**：监测每一层的梯度值，如果某一层的梯度值异常地大（远超过正常范围），可能是梯度爆炸的迹象。

3. **使用梯度裁剪**：梯度爆炸问题可以通过梯度裁剪（gradient clipping）来缓解。这意味着限制梯度的最大值，以防止它们变得太大。
	1. **选择阈值**：首先，需要选择一个梯度阈值（通常是一个正数），该阈值决定了梯度的上限值。典型的梯度裁剪阈值值为1.0或更小，但具体的数值可以根据问题和模型的需求进行调整。
    
	2. **计算梯度**：在正向传播和反向传播过程中，模型的梯度值会被计算。这些梯度值用于权重更新。
	    
	3. **裁剪梯度**：在反向传播过程中，当梯度值计算完毕后，梯度裁剪操作会被应用于梯度。具体而言，对于每个参数的梯度向量，它将被裁剪为不超过预先选择的阈值的大小。这通常通过以下步骤来实现：
	    
	    - 计算梯度向量的L2范数（Euclidean范数），即梯度向量的长度。
	    - 如果L2范数超过了阈值，将梯度向量缩放到一个新的长度，使其等于阈值。
	    
	    这可以通过以下公式实现： new_gradient=thresholdL2_norm×old_gradientnew_gradient=L2_normthreshold​×old_gradient
	    
	4. **权重更新**：裁剪后的梯度被用于更新神经网络的权重。这些更新通常使用随机梯度下降（SGD）或其他优化算法来执行。

5. **初始化参数**：选择合适的权重初始化方法，如Xavier初始化，可以帮助减小梯度爆炸的风险。

通常，梯度消失和梯度爆炸问题在深度神经网络中出现的几率较高。判断它们的存在需要观察损失曲线和梯度统计，同时采取相应的解决方法，如改变激活函数、使用梯度裁剪、合适的初始化等。在实践中，通常需要不断调整和优化模型来解决这些问题。


#### 给模型添加约束
要给模型添加约束目标，通常可以采用以下方法：

1. **Penalty Term**（惩罚项）：
   通过在损失函数中添加额外的惩罚项，可以强制模型满足某些约束。具体来说，如果你希望模型满足某些约束条件，你可以将一个与这些条件违反程度相关的惩罚项添加到损失函数中。例如，你可以使用L1或L2正则化来约束权重，或者使用限制条件的**Lagrangian乘子**方法来强制约束。

2. **约束函数**：
   可以在模型结构中引入专门的约束函数，以确保模型的输出满足某些条件。这些约束函数可以包括等式约束或不等式约束。例如，你可以为神经网络的输出添加一个**额外的层**，然后在该层应用**激活函数**，以确保输出满足某些约束条件。

3. **自定义损失函数**：
   你可以编写自定义损失函数，将其添加到模型训练中。这样，你可以根据需要对模型的输出进行约束。自定义损失函数通常在训练过程中通过梯度下降来优化，以使模型满足约束条件。

4. **优化算法**：
   一些优化算法具有内置的支持约束的能力。例如，投影梯度法（Projected Gradient Descent）可以用于在训练期间强制约束参数在某个范围内。这种方法特别适用于具有参数约束的问题。

5. **非线性规划**：
   对于具有复杂约束的问题，可以考虑使用非线性规划方法，这些方法专门用于求解具有约束的优化问题。非线性规划库如Scipy中提供了工具来处理这类问题。

#### 正则化

1. **L1正则化（L1 Regularization）**：
   - L1正则化引入了模型参数的绝对值之和，通过最小化以下损失函数来约束模型参数：
     $$\text{Loss} + \lambda \sum_{i=1}^{n} |w_i|$$
     其中，$w_i$ 是模型的参数，$\lambda$ 是正则化强度。
   - L1正则化推动模型学习稀疏特征，因为它倾向于将某些特征的权重设为零，从而进行特征选择。

2. **L2正则化（L2 Regularization）**：
   - L2正则化引入了模型参数的平方之和，通过最小化以下损失函数来约束模型参数：
     $$\text{Loss} + \lambda \sum_{i=1}^{n} w_i^2$$
   - L2正则化有助于避免模型的权重过大，从而降低过拟合风险，使模型更稳定。

3. **弹性网络正则化（Elastic Net Regularization）**：
   - 弹性网络是L1和L2正则化的组合，它的损失函数既包括L1项又包括L2项。它的目的是在特征选择和权重缩减之间取得平衡。

4. **早停法（Early Stopping）**：
   - 早停法是一种简单而有效的正则化方法，它通过监控模型在验证集上的性能，当性能不再提高时停止训练。这可以防止模型在训练数据上过拟合。

5. **丢弃法（Dropout）**：
   - 丢弃法是一种正则化技术，用于深度学习中的神经网络。它在每个训练批次中随机丢弃一定比例的神经元，从而减少神经元之间的依赖关系，降低过拟合风险。

6. **批标准化（Batch Normalization）**：
   - 批标准化是一种正则化技术，它通过标准化每个批次的输入数据，有助于加速训练并减少过拟合。

为什么l1能产生稀疏值 为什么l2平滑数据？
1. 正则项求导之后的效果，l1得到常数，l2得到一个一阶变量。传播过程一个可能得到0，一个仅平滑
2. 二维图像层面

Batch Normalization（BN）和Layer Normalization（LN）都是用于对神经网络层的输入进行归一化的技术。虽然它们都旨在解决训练期间的内部协变量漂移问题，但它们之间存在一些关键差异。

1. **归一化范围:**
   - **Batch Normalization（BN）：** 在整个批次维度B上进行归一化，考虑小批量中的所有示例。
   - **Layer Normalization（LN）：** 对每个示例C独立地在特征维度上进行归一化。

2. **训练与测试:**
   - **Batch Normalization（BN）：** 在训练期间，BN使用当前小批量的统计信息（均值和方差）进行归一化。在测试期间，通常使用在训练过程中收集的这些统计信息的运行平均值。
   - **Layer Normalization（LN）：** 它在训练和测试期间都独立地对每个示例进行归一化。不需要运行平均值。

1.same for all training examples

2.same for all feature dimensions

就对训练和测试的影响而言，BN和LN都有助于减少内部协变量漂移，稳定训练，并可能加速收敛。选择使用哪种取决于数据的特定特征，网络架构以及任务的性质。

总结一下：  
BN、LN可以看作横向和纵向的区别。  
经过归一化再输入激活函数，得到的值大部分会落入非线性函数的线性区，导数远离导数饱和区，避免了梯度消失，这样来加速训练收敛过程。

BatchNorm这类归一化技术，**目的就是让每一层的分布稳定下来**，让后面的层可以在前面层的基础上安心学习知识。

BatchNorm就是通过对batch size这个维度归一化来让分布稳定下来。LayerNorm则是通过对Hidden size这个维度归一。

**在神经网络中需要注意以下两点：**

（1）上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低。由于后层网络需要不停去适应输入数据分布的变化，会使得整个网络的学习速率过慢。

（2）网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度。当我们使用饱和激活函数(saturated activation function)时，例如sigmoid，tanh激活函数，很容易使得模型训练陷入梯度饱和区(gradient saturation zone)。

**BN的优点**

1. 可以使用更大的学习率，训练过程更加稳定，极大提高了训练速度。
2. 可以将bias置为0，因为Batch Normalization的Standardization过程会移除直流分量，所以不再需要bias。
3. 对权重初始化不再敏感，通常权重采样自0均值某方差的高斯分布，以往对高斯分布的方差设置十分重要，有了BatchNormalization后，对与同一个输出节点相连的权重进行放缩，其标准差σ也会放缩同样的倍数，相除抵消。
4. 对权重的尺度不再敏感，理由同上，尺度统一由γ参数控制，在训练中决定。深层网络可以使用sigmoid和tanh了，理由同上，BN抑制了梯度消失。
5. Batch Normalization具有某种正则作用，不需要太依赖dropout，减少过拟合。

## 为啥要做归一化

本质是对一批不太标准的数据统一到指定的格式。

一方面，激活函数有一定的饱和性，随着网络的深度增加，每层特征值分布会逐渐的向激活函数的输出区间的上下两端靠近，激活函数达到饱和，这样继续下去就会导致梯度消失。

BN可以将该层特征值分布重新**拉回标准正态分布**，特征值将落在激活函数对于输入较为敏感的区间，避免梯度消失，同时也可加快收敛。

另一个方面，从神经网络的效果来讲，神经网络的模型效果跟数据的分布有很大关系，机器学习领域有个很重要的假设：**IID独立同分布假设**，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。将**训练数据**和**测试数据**进行**归一化**，可以==避免数据分布不同对模型的影响==。
### batch size和learning rate

较大的批量通常需要较小的学习率，以避免模型在参数空间中跳过较好的区域。这是因为使用更大的批量可能导致更平滑的梯度，因此对参数的更新相对较大，需要减小学习率以保持稳定性


#### 再训练
1. FineTune
2. Adaptor
3. LoRA
- Adapters引入额外的推理延迟 (由于增加了模型层数)
- Prefix-Tuning难于训练，且预留给prompt的序列挤占了下游任务的输入序列空间，影响模型性能

BERT和GPT的区别

- **BERT：双向 预训练语言模型+fine-tuning（微调）**
- **GPT：自回归 预训练语言模型+Prompting（指示/提示）**

BERT模型虽然也是采用和GPT一样的Transformer模型结构，但它几乎就是为「无监督预训练+下游任务微调」的范式量身定制的模型。和GPT相比，BERT所使用的掩码语言模型任务（Masked Language Model）虽然让它失去了直接生成文本的能力，但换来的是双向编码的能力，这让模型拥有了更强的文本编码性能，直接的体现则是下游任务效果的大幅提升。而GPT为了保留生成文本的能力，只能采用单向编码。


##### Swin Transformer
- 编码方式略微不同
- Patch Merging的作用是分辨率减半，通道数加倍，类似于CNN的作用，在Transformer中实现Hierarchical。奇偶交叉分四块，concat（降低resolution），fully connect降通道
- W-MSA 分块做SA，降低运算量。但会出现分块隔离问题SW-MSA（shifted Windows Multi-Head Attention）


### 算法与数据结构
#### LRU
```python
class LRUCache:
    def __init__(self, capacity: int):
        self.cache = OrderedDict()
        self.capacity = capacity

    def get(self, key: int) -> int:
        if key not in self.cache:
            return -1
        self.cache.move_to_end(key)  # 将访问过的元素移动到末尾
        return self.cache[key]

    def put(self, key: int, value: int) -> None:
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = value
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False)  # 移除最早的元素

# 示例用法
cache = LRUCache(2)
cache.put(1, 1)
cache.put(2, 2)
print(cache.get(1))  # 返回 1
cache.put(3, 3)  # 移除 key 2
print(cache.get(2))  # 返回 -1 (未找到)

```

#### 红黑树
1. **节点颜色规则：**
    - 每个节点要么是红色，要么是黑色。
    - 根节点必须是黑色。
2. **节点关系规则：**
    - 红色节点的子节点必须是黑色（即没有连续的红节点）。
    - 从任意节点到其每个叶子节点的每条路径都必须包含相同数量的黑色节点，这被称为黑高度相同。
3. **插入规则：**
    - 插入的节点总是标记为红色。
    - 如果插入破坏了红黑树的性质，需要通过一系列的旋转和颜色变换来修复，以保持平衡。
4. **删除规则：**
    - 删除节点时，如果被删除节点有一个红色子节点，可以直接删除而不会破坏红黑树的性质。
    - 如果被删除节点是黑色且有一个黑色子节点，则需要通过一系列的旋转和颜色变换来修复树的平衡。
    - 删除节点时，如果被删除节点是黑色且有两个黑色子节点，也需要进行修复。

红黑树的平衡性质保证了树的高度不会过高，因此查找、插入和删除操作的平均时间复杂度都是O(log n)，其中n是树中节点的数量。这使得红黑树在许多应用中非常有用，如在C++的STL中的`std::map`和`std::set`的实现，以及数据库索引结构等。

需要注意的是，虽然红黑树确保了树的平衡，但它的平均性能比平衡二叉树（AVL树）略差，因为在插入和删除操作时可能需要更多的旋转。然而，红黑树的平均性能仍然足够高效，而且在动态插入和删除操作频繁的情况下，红黑树通常优于AVL树。

==为什么要红和黑？==标注两种颜色的目的是为了通过颜色规则来维护树的平衡性质，防止树变得过于不平衡，以保证高效的操作。基于：1.**红色节点的子节点必须是黑色**，2.**从任意节点到其每个叶子节点的每条路径都必须包含相同数量的黑色节点**。

#### 跳表
通过多层次的链表来组织数据，其中每一层都是原始链表的一部分，但具有不同的跨度。顶层链表是原始链表的一个子集，而底层链表包含所有元素。每一层都有一个头节点，尾节点，以及一些具有随机性的指针，这些指针用于快速跳过一些元素。
类似，使用二分去查找链表，类似平衡树AVL（红黑树）
**跳表（Skip List）：**

1. **插入（Insertion）：** 平均情况下，插入操作的时间复杂度为O(log n)，其中n是跳表中节点的数量。由于跳表的层次结构，插入操作可能涉及多个层次的更新。
    
2. **查找（Lookup）：** 平均情况下，查找操作的时间复杂度为O(log n)，因为跳表的层次结构允许跳过多个节点。
    
3. **删除（Deletion）：** 平均情况下，删除操作的时间复杂度为O(log n)，与插入和查找操作类似，可能需要更新多个层次的节点。

**哈希表（Hash Table）：**

1. **插入（Insertion）：** 平均情况下，插入操作的时间复杂度为O(1)，但在哈希冲突的情况下，可能需要O(n)的时间来解决冲突，其中n是哈希桶的大小。
    
2. **查找（Lookup）：** 平均情况下，查找操作的时间复杂度为O(1)，因为它可以通过哈希函数直接定位到所需的位置。
    
3. **删除（Deletion）：** 平均情况下，删除操作的时间复杂度为O(1)，但在哈希冲突的情况下，可能需要O(n)的时间来解决冲突，其中n是哈希桶的大小。
    

**平衡树（Balanced Tree，通常是红黑树）：**

1. **插入（Insertion）：** 平均情况下，插入操作的时间复杂度为O(log n)，其中n是树中节点的数量。由于平衡树需要维护平衡性，插入操作可能涉及树的旋转。
    
2. **查找（Lookup）：** 平均情况下，查找操作的时间复杂度为O(log n)，因为树的高度受到控制。
    
3. **删除（Deletion）：** 平均情况下，删除操作的时间复杂度为O(log n)，与插入操作类似，可能需要进行树的旋转来保持平衡。
    



### **深度学习的参数运算以及高性能计算问题**

|      arch       |                               Param                                |                                                                                     FLOPS                                                                                     |
| :-------------: | :----------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| self-attention  |                            $4h^2 + 4h$                             |                                                                       $4bs^2h$ (s 句子长，h embedding dim)                                                                        |
|       cnn       | $C_{o}\times (k^2\times  C_{i} + 1)$ 括号后面是一个卷积的参数量，总共有$C_{out}$个卷积 | $\left[\left(C_i \times k_w \times k_h\right)+\left(C_i \times k_w \times k_h-1\right)+1\right] \times C_o \times W \times H$ $->C_i \times k^2 \times C_o \times W \times H$ |
|       rnn       |                         $lnd^2$ (网络深度为$l$)                         |                                                                                                                                                                               |
| matrix multiply |                              $mn+np$                               |                                                                            $mnp$ -> opt $n^{2.3}$                                                                             |

#### Transformer的计算量参数量

<iframe 
		border=0
		frameborder=0
		height=550
		width=850
		src="https://zhuanlan.zhihu.com/p/625922438"></iframe>

随着batch_size的增大，前向传递和后向传递的FLOPs线性增长，而权重更新的FLOPs保持不变，参数更新的FLOPs变得可以忽略不计了。这体现为：**当batch_size足够大时，在训练神经网络的一次迭代中，前向传递和后向传递是主要的耗时环节，而参数更新的耗时变得几乎可以忽略不计**。

>前向传递和后向传递的计算量FLOPs与batch_size成正比，即随着batch_size增大而线性增长。
>优化器更新模型参数的FLOPs与batch_size无关，只与模型参数规模和优化器类型有关。
>随着batch_size增大，前向传递和后向传递的FLOPs线性增长，而权重更新的FLOPs保持不变，这使得权重更新的FLOPs变得逐渐可以忽略不计了



由于第一层的后向-前向FLOPs比率是1:1，而其他层后向-前向FLOPs比率是2:1。随着网络层数的加深，第一层对整体FLOPs的影响变得可以忽略不计了。这体现为：**当网络层数足够深时，后向传递的耗时几乎是前向传递耗时的2倍**。

总的来说，对于**用大batch_size的深层神经网络**来说，**后向传递-前向传递的FLOPs比率接近于2:1**，换句话说，**后向传递的计算量大约是前向传递的两倍**



#### 量化



### **操作系统杂项**

#### 死锁问题
死锁的四个条件是：

- **禁止[抢占](https://zh.wikipedia.org/wiki/%E6%8A%A2%E5%8D%A0%E5%BC%8F%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%A4%84%E7%90%86 "抢占式多任务处理")**（no preemption）：系统资源不能被强制从一个进程中退出。

- **持有和等待**（hold and wait）：一个进程可以在等待时持有系统资源。

- **[互斥](https://zh.wikipedia.org/wiki/%E4%BA%92%E6%96%A5%E9%94%81 "互斥锁")**（mutual exclusion）：资源只能同时分配给一个行程，无法多个行程共用。

- **循环等待**（circular waiting）：一系列进程互相持有其他进程所需要的资源。

避免死锁是一种重要的策略，可以通过设计和管理多进程或多线程系统来减少死锁的风险。以下是一些避免死锁的一般性方法：

1. **加锁顺序**：确保所有进程或线程都按照相同的顺序获取锁。这可以防止死锁，因为它消除了循环等待条件。
    
2. **使用超时**：如果一个进程或线程不能在合理的时间内获取所需的锁或资源，它可以释放已经占有的锁并等待一段时间后再次尝试。这可以防止死锁。
    
3. **资源分配图**：建立资源分配图，追踪每个进程或线程占有的资源和它们等待的资源。如果资源分配图中没有循环，那么系统是安全的。
    
4. **死锁检测和恢复**：实现死锁检测算法，定期检查系统状态以识别潜在的死锁。一旦检测到死锁，可以采取措施来终止其中一个或多个进程，以解除死锁。
    
5. **动态资源分配**：在资源分配时，动态调整资源分配以最小化死锁的风险。这可能需要在运行时分配和释放资源。
    
6. **有序资源分配**：规定资源的使用顺序，并确保进程或线程按照相同的顺序请求资源。
    
7. **避免长时间持有资源**：尽量减少进程或线程持有资源的时间，以减少死锁发生的机会。
    
8. **资源层次管理**：将资源分层管理，确保资源只能从低级别分配到高级别，而不是相反。
    
9. **资源抢占**：允许操作系统在必要时抢占资源，以解除死锁。
    
10. **使用锁的层次结构**：使用锁的层次结构，以确保进程或线程按照一定的顺序获取锁，而不是随机获取。

死锁检测方法：

1. **资源分配图**：资源分配图是死锁检测的一种直观方法。每个节点表示一个进程，每个资源分配点表示一个资源。图中的边表示进程对资源的请求和分配。通过检查资源分配图是否存在循环依赖，可以判断是否有死锁。如果没有循环依赖，系统是安全的；如果有循环依赖，可能存在死锁。
    
2. **银行家算法**：银行家算法是一种基于资源请求的死锁检测方法。它通过模拟进程的资源请求和释放，以检查是否存在死锁。如果系统可以满足所有进程的资源需求，那么系统是安全的；否则，可能存在死锁。
    
3. **等待图**：等待图是一种更简化的图形表示方法，用于表示进程和资源之间的关系。等待图通过检查等待链来检测死锁。如果等待图中存在环，就意味着可能存在死锁。
    
4. **资源分配矩阵**：资源分配矩阵是一种表示系统资源分配状态的矩阵。通过检查资源分配矩阵是否满足死锁条件，可以进行死锁检测。
    
5. **周期性检查**：定期执行死锁检测算法，以检查系统是否处于死锁状态。这种方法通常在系统资源较少的情况下使用，因为它的开销较大。
    
6. **超时和重试**：如果一个进程在等待资源的时间超过一定阈值，系统可以将其认为可能陷入死锁，并进行重试或终止。

#### 进程间通信
进程间通信（Inter-Process Communication，IPC）是不同进程之间进行数据交换和通信的方式。进程间通信是操作系统和多任务编程中的重要概念，它允许进程协作和共享数据。以下是一些常见的进程间通信方式：

1. **管道（Pipes）**：管道是一种单向通信方式，用于在一个进程写入数据，另一个进程读取数据。通常有两种类型：无名管道（在亲缘关系的进程之间使用）和命名管道（也称为FIFO，可以在不相关的进程之间使用）。
    
2. **消息队列（Message Queues）**：消息队列是一种进程间通信方式，允许一个进程向队列发送消息，另一个进程从队列中接收消息。消息队列通常用于进程之间的异步通信。
    
3. **共享内存（Shared Memory）**：共享内存是一种高效的进程间通信方式，允许多个进程共享同一块内存区域。这意味着进程可以直接读取和写入共享内存，而无需复制数据。共享内存通常需要额外的同步机制来避免数据冲突。
    
4. **信号（Signals）**：信号是一种异步通信方式，允许一个进程向另一个进程发送信号，通常用于通知目标进程发生了某个事件。例如，当用户按下 Ctrl+C 键时，终端会向正在运行的进程发送中断信号。
    
5. **套接字（Sockets）**：套接字是一种网络编程中常见的进程间通信方式。它允许进程在不同的计算机上通过网络进行通信。套接字通常用于分布式系统中。
    
6. **文件（File）**：进程可以通过读写共享的文件进行通信。这通常用于进程之间的持久性数据交换。
    
7. **信号量（Semaphores）**：信号量是一种用于进程同步的通信机制。它可以用来防止多个进程同时访问共享资源。

#### 进程、线程、协程

1. **进程（Process）**：
    - 进程是操作系统中的一个独立执行单元。每个进程都有自己的内存空间、文件句柄、寄存器等。
    - 进程之间相互隔离，一个进程的崩溃不会影响其他进程。
    - 进程之间通常通过进程间通信（IPC）来进行数据交换，如管道、消息队列、共享内存等。
    - 进程的创建和销毁比较耗费系统资源，因此通常用于实现独立的应用程序或服务。
2. **线程（Thread）**：
    - 线程是进程内的执行单元，多个线程共享同一个进程的内存空间和资源。
    - 线程之间可以相互通信和共享数据，但也需要注意同步和互斥，以避免竞争条件。
    - 线程的创建和销毁比进程轻量，因此常用于并发处理、多任务和提高程序性能。
3. **协程（Coroutine）**：
    用于**高度并发**的任务的语言，如 Golang的goroutine
    - 协程是一种**轻量级的线程**，它在同一个线程内的不同执行点之间切换，而不需要操作系统的支持。
    - 协程通常由程序员**显式控制**，可以随时切换执行点，实现非抢占式多任务。
    - 协程适用于**高度并发**的任务，如网络编程、异步编程、生成器等。

|功能|进程|线程|协程|
|---|---|---|---|
|切换|OS|OS|user|
