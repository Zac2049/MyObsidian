## 似然性
似然 -- 概率  意思相近，都指事件发生的可能性
概率，用于在已知一些参数的情况下，预测接下来在观测上所得到的结果；似然性，则是用于在已知某些观测所得到的结果时，对有关事物之性质的参数进行估值，也就是说已观察到某事件后，对相关参数进行猜测。
![[Pasted image 20230529083517.png]]
**交叉熵损失函数**的本质是**对数似然函数**的负数，它是一种测量两个**概率分布**之间相似度的方法，常用于分类问题中。最小化损失函数的过程本质上是在最大化模型预测结果的似然函数，即最大化在给定模型下，观测数据的条件概率密度函数的值。

最小化损失函数的过程本质上是在最大化模型预测结果的似然函数，即最大化给定模型下，观测数据的条件**概率密度函数**的值。

通过最小化损失函数来调整模型参数，本质上是在最大化观测数据的似然函数，即最大化在给定模型下，观测数据的条件概率密度函数的值。此外，很多深度学习模型都是基于**概率模型**构建的，比如生成对抗网络（GAN）、变分自编码器（VAE）等，这些模型的目标也是最大化观测数据的似然函数，即最大化在给定模型下，观测数据的条件概率密度函数的值。


- 将回归任务转化为分类任务的主要原因是，分类任务通常更易于建模和解释，并且在某些情况下可以提供更好的性能表现。以下是一些可能的原因：

1.  简单性：分类任务通常更简单明了，更容易解释和理解，因为输出是离散的类别而不是连续的数值。对于某些应用场景，离散的类别可能更符合实际需求，例如医疗诊断中的疾病分类、客户满意度的评价等。
    
2.  可解释性：分类任务的结果更容易解释，因为它们被映射到已知的类别标签上。相比之下，回归任务的结果通常更难解释，因为它们是连续的数值，与具体的应用场景可能没有直接的联系。
    
3.  精度：在某些情况下，将回归任务转化为分类任务可以提高模型的性能表现。例如，在一些预测问题中，输出变量的取值范围非常广泛，或者存在异常值时，回归模型可能会出现较大的误差。而将其转化为分类任务后，可以将输出离散化，使得模型更容易处理。
    
4.  数据平衡：在某些情况下，回归任务的数据分布可能不太均衡，而分类任务可以通过调整类别权重来解决这个问题。例如，在预测罕见事件的问题中，分类任务可以通过增加罕见事件的权重来提高模型对其的预测准确性。
    
5.  模型选择：分类任务通常有更多的模型选择，例如决策树分类、支持向量机、神经网络等，这些模型在分类任务中已经被广泛应用和优化，可以直接应用于回归任务的转化中。


### 多项式分布和多重伯努利分布

![[Pasted image 20230731161229.png]]

多项分布（Multinomial Distribution）和多重伯努利分布（Multivariate Bernoulli Distribution）都是概率分布，用于描述多个随机变量的分布情况，但它们有着不同的特点和应用场景。

1. **多项分布（Multinomial Distribution）**：
   - 多项分布描述了在一次实验中，有多个可能结果的情况下，各个结果出现的次数的概率分布。典型的例子是掷骰子，每次掷骰子可以得到1到6的一个数字，多项分布描述了每个数字出现的次数的概率。
   - 用数学表示，如果有 $k$ 个可能的结果，每个结果的概率为 $p_1, p_2, ..., p_k$，那么多项分布可以表示为：
     $P(X_1 = x_1, X_2 = x_2, ..., X_k = x_k) = \frac{n!}{x_1!x_2!...x_k!} p_1^{x_1} p_2^{x_2} ... p_k^{x_k}$
     其中，$n$ 是实验的总次数，$x_1, x_2, ..., x_k$ 是每个结果出现的次数，满足 $x_1 + x_2 + ... + x_k = n$，且 $\sum_{i=1}^{k} p_i = 1$。



2. **多重伯努利分布（Multivariate Bernoulli Distribution）**：
   - 多重伯努利分布描述了多个二元随机变量的联合分布，每个随机变量只有两种可能的取值。典型的例子是对于一个包含多个二元特征的数据集，多重伯努利分布描述了每个特征取值为0或1的情况。
   - 用数学表示，如果有 $n$ 个二元随机变量，每个变量取值为0或1的概率分别为 $p_1, p_2, ..., p_n$，那么多重伯努利分布可以表示为：
     $P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n) = \prod_{i=1}^{n} p_i^{x_i} (1 - p_i)^{1 - x_i}$
     其中，$x_i$ 取值为0或1，表示第 $i$ 个变量的取值。

总的来说，多项分布适用于描述多个结果的离散型随机变量的分布情况，而多重伯努利分布适用于描述多个二元随机变量的联合分布情况。




- 多分类问题是不是就是预测一个多重伯努利分布？

不完全正确。多分类问题是指将输入样本分为多个不同的类别，而不是预测一个多重伯努利分布。

在多分类问题中，我们的目标是为每个输入样本分配一个正确的类别标签。这意味着我们需要建立一个模型，该模型可以从输入数据中学习到每个类别的特征，并根据这些特征进行分类预测。常见的多分类算法包括逻辑回归、支持向量机、决策树、随机森林、神经网络等。

与之不同的是，多重伯努利分布是指多个独立的伯努利分布的组合。伯努利分布是一种二元随机变量的概率分布，它的取值只有两个可能的结果，比如成功或失败，1或0。多重伯努利分布将多个伯努利分布组合在一起，每个伯努利分布代表一个特征或属性的二元取值。在这种情况下，我们的目标是预测每个特征的二元取值，而不是将输入样本分为多个类别。

因此，多分类问题和多重伯努利分布是两个不同的概念，它们涉及到不同的问题和建模方法。

- “One example of a function that must be stabilized against underﬂow andoverﬂow is thesoftmax function. The softmax function is often used to predictthe probabilities associated with a multinoulli distribution.”  根据这句话，我们每次做分类问题都会 用softmax，这里说的是多重伯努利。所以我提问他们是否相同

在给定的引述中，提到了softmax函数用于预测与多项分布相关的概率。多项分布是一种离散概率分布，它表示在多个离散结果中的每个结果上观察到的次数或概率。与多分类问题相关的是多项分布，而不是多重伯努利分布。

softmax函数通常用于多分类问题中，它将一系列实数转换为一个概率分布向量，其中每个元素表示一个类别的概率。这样，我们可以选择具有最高概率的类别作为预测结果。softmax函数的输出确保了所有类别的概率之和为1，因此可以解释为多项分布的概率。

另一方面，多重伯努利分布是指多个独立的伯努利分布的组合，每个伯努利分布代表一个特征或属性的二元取值。多重伯努利分布通常用于处理二元特征的多分类问题。

因此，softmax函数用于多分类问题，与多项分布相关，而多重伯努利分布通常用于处理二元特征的多分类问题。两者是不同的概念，不同的问题和建模方法。希望这次的解释更加明确。

### 最大似然估计

**最大似然估计**：在某些情况下，MSE可以与最大似然估计（MLE）相联系。对于高斯分布误差模型（即假设误差服从高斯分布），MSE最大化等价于最大似然估计。

Q: 极大似然估计和梯度下降有什么区别？

A:
极大似然估计（MLE）和梯度下降是两个不同的概念，它们解决不同类型的问题，尽管它们都涉及参数优化。

1. **问题类型**：
    - 极大似然估计（MLE）用于参数估计问题。它是一种用于估计总体参数的方法。给定样本数据和总体概率密度函数，MLE寻找使得观测样本出现的可能性最大的参数值。
    - 梯度下降是一种优化算法，用于求解无约束优化问题。它不涉及概率分布或参数估计，而是用于找到函数的最小值或最大值。
2. **优化目标**：
    - 极大似然估计的目标是最大化似然函数或对数似然函数，从而找到使观测样本出现的可能性最大的参数值。
    - 梯度下降的目标是最小化一个损失函数或成本函数，通过迭代的方式沿着梯度的反方向更新参数，直到达到损失函数的最小值。
3. **迭代过程**：
    - 极大似然估计通常通过解析求解或数值优化的方式来得到最优参数，而不需要进行迭代过程。
    - 梯度下降是一个迭代算法，它从一个初始的参数值开始，不断地沿着梯度的反方向更新参数，直到收敛到损失函数的最小值。
4. **问题约束**：
    
    - 极大似然估计通常没有显式的约束，参数可以取任意实数值。
    - 梯度下降用于解决无约束优化问题，因此参数可以取任意实数值。如果有约束条件，需要使用特定的优化算法（如约束优化算法）。



Q:极大似然估计和损失函数的设计有什么关系？

A:
极大似然估计（MLE）是一种用于估计总体参数的方法，它通过最大化似然函数或对数似然函数来找到使观测样本出现可能性最大的参数值。在极大似然估计中，似然函数表示了观测样本出现的可能性，而参数是待估计的未知量。为了获得参数的估计值，我们需要选择适当的概率分布模型和对应的似然函数。

在机器学习中，我们通常定义一个损失函数（Loss Function），用于衡量模型的预测值与真实值之间的差异。损失函数的设计通常与所采用的模型有关。比如，在线性回归中，常见的损失函数是均方误差（Mean Squared Error，MSE），在逻辑回归中，常见的损失函数是对数损失（Log Loss）。

这里的关系在于，当我们采用特定的概率分布模型和似然函数时，**通过最大似然估计得到的参数估计值，实际上就是使得我们选择的损失函数最小化的结果**。因此，最大似然估计可以看作是一种损失函数的优化过程，通过最大化似然函数来最小化损失函数。

同时，很多常用的损失函数，特别是在监督学习中，也可以与极大似然估计建立联系。例如，对数损失在逻辑回归中的使用与伯努利分布的似然函数密切相关，均方误差在高斯分布误差模型中与高斯分布的似然函数对应。

因此，极大似然估计和损失函数的设计在一定程度上是相关的，特别是当我们通过最大似然估计来获得参数估计值，并将似然函数与特定损失函数联系起来时。这种关系有助于我们理解似然函数和损失函数之间的联系，并帮助我们选择合适的损失函数和模型来解决特定的机器学习问题。


the function estimator $\hat{f}$ is simply a point estimator in function space.


**方差的含义：方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。**

**偏差的含义：偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。**

- **高偏差低方差模型**：这种模型对数据进行了过度简化，导致欠拟合。它们通常表现为训练数据和测试数据上的性能都不佳。
    
- **低偏差高方差模型**：这种模型复杂，对训练数据过度敏感，导致过拟合。它们可能在训练数据上表现良好，但在测试数据上性能较差。



为什么高斯分布的方差的无偏估计不是平方差除个数的形式

在统计学中，高斯分布（也称为正态分布）的方差的无偏估计通常使用 "平方差除以个数减1" 的形式，而不是简单的 "平方差除以个数" 的形式。

首先，让我们回顾一下方差的定义。对于一个具有 \(n\) 个样本的数据集，其方差被定义为样本数据与样本均值之间差的平方的平均值。形式化表示为：

$[ \text{方差} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2 ]$

其中，\(x_i\) 是第 \(i\) 个样本数据，\(\bar{x}\) 是样本的均值。

然而，使用上面的表达式计算的方差是有偏估计。这是因为计算方差时我们用到了样本均值 \(\bar{x}\)，而不是总体均值。由于样本均值是根据样本数据计算得到的，它在一定程度上偏离了真实的总体均值。因此，使用样本均值来估计方差会导致方差估计的偏差。

为了消除方差估计的偏差，我们在计算方差时采用了 "平方差除以个数减1" 的形式，也称为无偏估计。它是通过将方差除以自由度（自由度为 \(n-1\)）来调整偏差的。

$[ \text{无偏方差估计} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2 ]$

这个形式的方差估计是无偏的，意味着在大样本情况下，它的期望值等于真实的总体方差。这使得无偏方差估计成为常用的统计学工具，特别在样本容量较小的情况下，使用无偏方差估计可以得到更准确的估计结果。

首先如何估计参数，点估计，函数估计，极大似然估计；然后参数估计是否无偏（偏不偏就是指bias怎么样就是参数的期望和参数真实值相减），计算参数的bias，所以方差的无偏估计为什么要多减一，然后算置信区间，根据置信水平样本容量计算，计算Variance，或者说SE，样本方差是偏差平方和除数量-1；之后，进行bias和variance的trade off，根据
$[ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - y_i^{(\text{true})})^2 ]$
衡量


解释最大似然估计的一种方法是将其视为最小化由训练集和模型分布定义的经验分布 $\hat{p}_{data}$ 之间的差异，以及通过 KL 散度测量的两者之间的差异程度。

即最大化似然函数等价于最小化负对数似然，等价于最小化经验分布和模型分布的KL散度，或者说交叉熵，因为交叉熵就是KL散度减去真实分布的自信息。

许多作者使用术语“交叉熵”来具体标识伯努利或 softmax 分布的负对数似然，但这是用词不当。任何由负对数似然组成的损失都是训练集定义的经验分布与模型定义的概率分布之间的交叉熵。例如，均方误差是经验分布与高斯模型之间的交叉熵。
因此，我们可以将最大似然视为使模型分布与经验分布相匹配的尝试。

当示例数量小到足以产生过度拟合行为时，可以使用**正则化策略（例如权重衰减）**
来获得最大似然的有偏差版本，当训练数据有限时，该版本的方差较小。换种说法，weight decay 就是在数据量小的情况下使用的最大似然的有偏差版本。

关于内积和特征的讨论：在某些无限维空间中，我们需要使用其他类型的内积，例如基于积分而不是求和的内积。




- 鞍点视为沿着成本函数的一个横截面的局部最小值和沿着另一个横截面的局部最大值。


## Normalization

Batch Normalization (BN) 的新理解认为，BN 可以通过减少内部协变量偏移（Internal Covariate Shift）来平滑损失函数的landscape。内部协变量偏移是指在网络的每一层，输入分布的改变导致了网络参数的变化，从而使得网络在训练过程中需要不断地适应新的输入分布，这可能增加了训练的难度。BN通过标准化每个批次的输入，使得每一层的输入分布保持稳定，从而减少了内部协变量偏移的影响，使得损失函数的landscape更加平滑，有助于优化算法更稳定地收敛到全局最优解或者更好的局部最优解