## 240131 数坤

- ==部分标注问题==中 将one hot 设为输入是否有必要（感觉没有
- dice loss 和 ce loss哪个更能解决小物体分割问题，dice loss集中于计算各个物体的交并情况，是各个比值的平均，ce loss其实是像素级的概率分布的偏差，本身对小物体不利。但我们既要dice loss也要ce loss因为dice loss虽然也能反映两个分布的差异情况，但会出现失之毫厘差之千里的情况，ce loss能弥补这点
- dice loss实际是拿概率值进行计算，而IoU loss是用sigmoid或者softmax后的离散值进行计算，本身有一些平滑的效果。（建议再详细看看这两者的差别
### CLIP驱动的分割问题

关于prompt的问题，prompt应该如何设计，不同的prompt会给出不同的pred。
>这是个复杂的问题，因余力不足，我的工作仅限于做简单的实验，展现不同prompt的影响

为什么有效？
我的观点，预训练的CLIP准确，提供标签间的语义信息。（问：标签之间的语义信息也可以分化标签来解决，为什么还需要这个）。（问：这些语义信息能多大提升模型的精度？答：实验体现）

text encoder和image encoder 是不是能同时frozen，结合那几篇论文想想

## 240220图灵量子

`watch -n 10 nvidia-smi` 每隔十秒动态周期刷新结果

### K means

K均值（K-Means）是一种常用的聚类算法，用于将数据集划分为K个不同的簇。以下是K均值的具体计算过程：

1. **初始化：** 随机选择K个数据点作为初始簇的中心（质心）。

2. **分配：** 对每个数据点，计算其与K个质心的距离，将其分配到距离最近的簇。

3. **更新：** 对每个簇，计算其所有成员的平均值，将该平均值作为新的质心。

4. **迭代：** 重复步骤2和步骤3，直到质心的变化小于某个阈值，或者达到预定的迭代次数。

下面以一个简单的例子来说明K均值的计算过程：

假设有以下数据集：

\[ X = \{(2, 3), (5, 3), (1, 1), (8, 5), (7, 2), (6, 4), (2, 9), (4, 8)\} \]

我们想要将这些点分为两个簇（K=2）。

**Step 1: 初始化**
随机选择两个数据点作为初始质心，比如选择 (2, 3) 和 (5, 3)。

**Step 2: 分配**
计算每个数据点到两个质心的距离，将其分配到最近的簇：

- 簇1: $(2, 3), (1, 1), (2, 9), (4, 8)$
- 簇2: $(5, 3), (8, 5), (7, 2), (6, 4)$

**Step 3: 更新**
计算每个簇的新质心：

- 新质心1: $(2.25, 5.25)$
- 新质心2: $(6.5, 3.5)$

**Step 4: 迭代**
重复进行步骤2和步骤3，直到质心的变化小于某个阈值，或者达到预定的迭代次数。

这样，经过几轮迭代，K均值算法会不断调整质心，最终将数据划分成两个簇。这是一个简化的例子，实际应用中可能需要多次运行算法，因为初始质心的选择可能影响最终的聚类结果。





### Adam的学习率调整策略，学习率是根据梯度调整的，具体根据其一阶矩和二阶矩

截止到我知识截断日期（2022年1月），以下是一些最新和最常用的优化器以及它们的原理。请注意，领域和研究的发展可能导致新的优化器的出现或一些方法的更新。

1. **Adam (Adaptive Moment Estimation):**
   - **原理：** Adam结合了梯度的一阶矩估计（梯度的指数移动平均）和二阶矩估计（梯度平方的指数移动平均）。它具有自适应学习率，使得对于不同参数的梯度可以使用不同的学习率。以及考虑==动量==，及过去的梯度对现在的梯度更新有一个势能，惯性
   - **公式：** $m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$, $v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$, $\hat{\alpha}_t = \frac{\alpha \sqrt{1 - \beta_2^t}}{1 - \beta_1^t}$, $w_{t+1} = w_t - \hat{\alpha}_t \cdot \frac{m_t}{\sqrt{v_t} + \epsilon}$.
   - **备注：** $\beta_1$ 和 $\beta_2$ 是衰减因子，$\epsilon$ 是平滑项，$\alpha$ 是初始学习率。

2. **SGD (Stochastic Gradient Descent):**
   - **原理：** SGD是最基本的优化算法，它在每一步使用整个训练集中的一个随机样本来更新模型参数。随机性有助于逃离局部最小值，但可能导致参数更新的不稳定。
   - **公式：** $w_{t+1} = w_t - \eta \cdot g_t$, 其中 $g_t$ 是在时间步 $t$ 的梯度，$\eta$ 是学习率。

3. **RMSprop (Root Mean Square Propagation):**
   - **原理：** RMSprop也是一种自适应学习率的算法，它使用梯度平方的==指数移动平均==来调整学习率。相比于Adam，RMSprop没有使用一阶矩估计。
   - **公式：** $v_t = \beta \cdot v_{t-1} + (1 - \beta) \cdot g_t^2$, $w_{t+1} = w_t - \frac{\eta}{\sqrt{v_t} + \epsilon} \cdot g_t$, 其中 $\beta$ 是衰减因子，$\epsilon$ 是平滑项。

4. **AdaGrad (Adaptive Gradient Algorithm):**
   - **原理：** AdaGrad是一种自适应学习率的算法，它根据每个参数的历史梯度平方和调整学习率，对于频繁出现的梯度较小的参数，学习率会较大。
   - **公式：** $s_t = s_{t-1} + g_t^2$, $w_{t+1} = w_t - \frac{\eta}{\sqrt{s_t} + \epsilon} \cdot g_t$, 其中 $s_t$ 是历史梯度平方和。

是的，AdamW是对Adam优化算法的一种改进。它主要通过对==权重衰减（Weight Decay）的处理方式进行调整，以减少对模型权重的正则化影响==。

在Adam中，权重衰减通常是通过在梯度更新时简单地添加一个与==权重成比例的项==来实现的。但这种方法可能会导致对学习率的==过度惩罚==，尤其是在训练过程的早期阶段。

AdamW提出了一种更为合理的权重衰减方式，它将权重衰减的项添加到损失函数的计算中，而不是梯度更新的计算中。具体而言，对于一个具有权重衰减超参数 $\lambda$ 的参数 $w$，AdamW的损失计算方式如下：

$$ L_{\text{AdamW}} = L_{\text{original}} + \frac{1}{2} \cdot \lambda \cdot \|w\|_2^2 $$

其中，$L_{\text{original}}$ 是原始的损失函数。这样做的目的是让权重衰减的影响更显式地体现在损失函数中，而不是直接作用于梯度更新。

总的来说，AdamW相较于Adam的改进主要集中在对权重衰减的处理上，==旨在减小权重衰减对学习率的不必要惩罚==，从而更有效地优化模型。在一些实验中，AdamW在一些任务上的性能优于传统的Adam算法。


IoU计算，一般不需要计算前景后景，计算目标框和真实框

设计模式

有时候缺少驱动力

## 240221笔试

AI算法工程师是否存在某个题库

回溯问题，出了几个

KMP
"前缀"指除了最后一个字符以外，一个字符串的全部头部组合；"后缀"指除了第一个字符以外，一个字符串的全部尾部组合。

　"next值"就是"前缀"和"后缀"的最长的共有元素的长度。以"ABCDABD"为例，

－　"A"的前缀和后缀都为空集，共有元素的长度为0； 　　
－　"AB"的前缀为[A]，后缀为[B]，共有元素的长度为0； 　　
－　"ABC"的前缀为[A, AB]，后缀为[BC, C]，共有元素的长度0； 　　
－　"ABCD"的前缀为[A, AB, ABC]，后缀为[BCD, CD, D]，共有元素的长度为0； 　　
－　"ABCDA"的前缀为[A, AB, ABC, ABCD]，后缀为[BCDA, CDA, DA, A]，共有元素为"A"，长度为1； 　　
－　"ABCDAB"的前缀为[A, AB, ABC, ABCD, ABCDA]，后缀为[BCDAB, CDAB, DAB, AB, B]，共有元素为"AB"，长度为2； 　　－　"ABCDABD"的前缀为[A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为[BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为0

## 240228百度
一些linux命令
sed ？ find kill
shell grep awk sed三剑客

python list del remove pop
- pop 可以按索引，只pop一个，返回值
- del 按索引按切片删除
- remove 按值删除

c++内存模型
- **栈（Stack）：** 用于存储局部变量和函数调用的信息。栈上的数据在函数执行结束后会被自动释放。
- **堆（Heap）：** 用于动态分配内存，程序员手动分配和释放。不同于栈，堆上的数据的生命周期需要程序员管理。
- **全局/静态存储区（Global/Static Storage Area）：** 用于存储全局变量和静态变量，这些变量在整个程序的执行过程中都存在。

当涉及到在Unix-like操作系统中进行文本处理时，`grep`、`awk`和`sed`是强大的命令行工具。


#### 僵尸进程处理
僵尸进程是已经终止但其父进程尚未调用`wait`或`waitpid`来获取其退出状态的进程。处理僵尸进程的方法通常包括以下几个步骤：

1. **找到僵尸进程：** 使用 `ps aux | grep Z` 或 `ps -ef | grep Z` 命令可以查找系统中的僵尸进程。僵尸进程通常会显示为状态栏（STAT）中的 "Z"。

2. **找到父进程：** 查找僵尸进程的父进程ID（PPID）。可以使用 `ps -o ppid= -p <zombie_pid>` 命令来查找僵尸进程的父进程ID。

3. **终止父进程：** 如果僵尸进程的父进程不再需要该进程的信息，可以尝试终止父进程。请注意，这可能会影响父进程正在执行的任务，因此需要谨慎操作。

4. **重启系统：** 如果僵尸进程的父进程无法终止或无法找到，可以考虑重启系统。系统重启后，所有僵尸进程都会被清理。

请注意，僵尸进程本身不会占用系统资源，因为它们已经终止。但是，如果系统中有太多的僵尸进程，可能会导致进程表被填满，从而影响系统性能。

如果发现僵尸进程的出现是由于父进程没有及时调用`wait`或`waitpid`导致的，可以修改父进程的代码，确保它及时处理子进程的退出状态。这可以防止僵尸进程的产生。

总的来说，处理僵尸进程通常需要检查父进程、终止不再需要的父进程或者通过重启系统来清理。


### find
- 通过扩展名查找文件：

`find 指定目录 -name '*.ext'`

- 查找匹配多个路径或名称模式的文件：

`find 指定目录 -path '**/path/**/*.ext' -or -name '*pattern*'`

- 查找匹配指定名称的目录，不区分大小写：

`find 指定目录 -type d -iname '*lib*'`

- 查找匹配指定模式的文件，排除特定路径：

`find 指定目录 -name '*.py' -not -path '*/site-packages/*'`

- 查找符合指定大小范围的文件，将递归深度限制为 "1"：

`find 指定目录 -maxdepth 1 -size +500k -size -10M`

- 对每个文件运行命令（在命令中使用 `{}` 代表当前文件）：

`find 指定目录 -name '*.ext' -exec wc -l {} \;`

- 查找最近 7 天修改的文件：

`find 指定目录 -daystart -mtime -7`

- 查找空（0 字节）的文件并删除：

`find 指定目录 -type f -empty -delete`

### `grep`（全局正则表达式打印）

- **用途：** `grep`用于在文本中搜索和匹配模式。它根据是否包含指定模式来过滤文本文件或输出的行。

- **基本用法：**
  ```bash
  grep 模式 文件名
  ```

- **例子：**
  ```bash
  grep "error" log.txt
  ```
  这个命令会显示`log.txt`文件中包含单词"error"的所有行。

### `awk`

- **用途：** `awk`是一种用于文本处理的多功能编程语言。它特别适用于处理和分析结构化文本数据，如表格中的列。

- **基本用法：**
  ```bash
  awk '模式 { 动作 }' 文件名
  ```

- **例子：**
  ```bash
  awk '{ print $2 }' data.txt
  ```
  这个命令打印`data.txt`文件中每行的第二列。

### `sed`（流编辑器）

- **用途：** `sed`是一种流编辑器，用于在输入流（文件或管道输入）上执行基本文本转换。它经常用于搜索和替换操作。

- **基本用法：**
  ```bash
  sed 's/模式/替换/g' 文件名
  ```

- **例子：**
  ```bash
  sed 's/旧/新/g' input.txt > output.txt
  ```
  这个命令在`input.txt`文件中替换所有出现的"旧"为"新"，并将结果写入`output.txt`。


## 20240923小米

prompt engineering

Q：会以什么方式利用chat gpt解决问题

Q：遇到问题怎么解决，比如hdr的问题

Q：transformer中的mask attention 具体怎么计算的

Q：介绍下bert

Q ：vit中的图像块是怎么映射到embedding的

在 Vision Transformer (ViT) 中，将图片分块并嵌入 (embedding) 的过程是将图像切割成多个小块（patches），然后通过线性变换将每个图像块转换为固定大小的向量。这些向量就是这些块的嵌入表示，之后再加上位置编码，送入 Transformer 模型进行进一步处理。

以下是 ViT 将图片分块并嵌入的步骤：

1. 图像分块 (Patch Embedding)
首先，将输入图像（例如大小为 \( H \times W \times C \)，即高、宽和通道数）切割成不重叠的固定大小的图像块 (patches)。假设每个图像块的大小为 \( P \times P \)，那么可以从图像中提取出 \( \frac{H}{P} \times \frac{W}{P} \) 个块。

具体步骤如下：
- 将输入图像按照 \( P \times P \) 大小的窗口分块，每个图像块大小为 \( P \times P \times C \)。
- 将每个图像块展平为一个向量，展平后的向量大小为 \( P^2 \times C \)。

2. 线性变换 (Linear Projection)
将展平后的图像块向量通过一个线性变换（通常是一个可学习的矩阵），将其映射到一个更高维的空间。具体是：
- 使用一个可学习的权重矩阵 \( W \in \mathbb{R}^{(P^2 \cdot C) \times D} \)，将每个展平的图像块向量（大小为 \( P^2 \cdot C \)）变换为维度为 \( D \) 的向量，\( D \) 是 Transformer 模型的嵌入维度。

3. 加入位置编码 (Position Embedding)
由于 Transformer 是无序的，即它并不能隐式地知道输入的顺序，因此需要引入位置编码。位置编码是一种固定的或可学习的向量，将每个图像块的相对位置信息加入到嵌入中。

- 位置编码是一个可学习的向量矩阵 \( \text{PosEmb} \in \mathbb{R}^{N \times D} \)，其中 \( N = \frac{H}{P} \times \frac{W}{P} \) 是图像块的数量，\( D \) 是嵌入维度。这个位置编码会与图像块的嵌入向量相加。

4. 特征组合
将每个块的嵌入向量与其对应的位置信息加和后，形成一个完整的序列，这个序列作为 Transformer 的输入。

代码示例
以下是一个简化的 PyTorch 实现，展示如何进行图片分块和嵌入：

```python
import torch
import torch.nn as nn

class ViTEmbedding(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):
        super(ViTEmbedding, self).__init__()
        self.patch_size = patch_size
        # 图像块的数量
        self.num_patches = (img_size // patch_size) * (img_size // patch_size)
        # 线性投影矩阵，用于将每个图像块嵌入为 embed_dim 维度
        self.proj = nn.Linear(patch_size * patch_size * in_channels, embed_dim)
        # 位置编码
        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches, embed_dim))

    def forward(self, x):
        B, C, H, W = x.shape
        # 将图像按照 patch_size 切分成块，并展平
        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)
        x = x.contiguous().view(B, C, -1, self.patch_size * self.patch_size).permute(0, 2, 1, 3)
        x = x.contiguous().view(B, -1, self.patch_size * self.patch_size * C)
        
        # 线性投影到嵌入维度
        x = self.proj(x)
        
        # 加入位置编码
        x += self.pos_embed
        return x

# 示例
img = torch.randn(1, 3, 224, 224)  # 输入图像，大小为 224x224，3 个通道
vit_embedding = ViTEmbedding()
out = vit_embedding(img)
print(out.shape)  # 输出形状为 (1, num_patches, embed_dim)
```

总结步骤
1. **图像分块**：将图像分割成固定大小的块。
2. **展平图像块**：将每个图像块展平为一维向量。
3. **线性变换**：将展平的图像块通过线性变换映射到嵌入维度。
4. **位置编码**：将位置信息加入每个块的嵌入中。
5. **送入 Transformer**：将嵌入序列作为输入传递给 Transformer 模型。

这就是 ViT 中图片分块到嵌入的全过程。
## 20240926天天想学

Q：跟踪reid问题一般用什么指标？
精度accuracy 跟踪目标的位置与真实位置的接近程度，距离
多目标跟踪准确率MOTA 综合考虑了漏检、误检以及ID交换
ID 变换率
多目标跟踪精度MOTP 衡量检测框与真实目标位置的匹配程度，计算的是预测目标和真实目标间的平均距离
精确率 召回率 也用，但是不多
## 20241009华为

Q：为什么从上一家离职？

主要两个原因。
沟通上，mentor并未充当mentor的职位，或者说不符合mentor导师这个称呼，更像是监工，指令下发者，工作情况的监督者。
技术安排上，安排的任务基本由我一人完成，包括mentor及其他帮助以及反馈上表现一般，往往是一个不成熟的方案和一句命令，让我完成一个项目或者一个feature，并且这个方案和命令并不详实和明确，并在我完成这个项目时，本人积极交流但他人给的正反馈并不多，过程中许多甚至起到负反馈的效果。

应该回答：公司的这个项目结束了，被安排十分边缘的项目，我觉得对我的职业发展意义有限。

Q：什么情况下会离职？
有了合适的机会，个人是不想离职的。公司信任我，给我平台我是十分愿意继续耕耘的。


## 20241015 无问智行

Q：算法题，list的排列问题

list的list如何去重？

1. 利用set和tuple去重
```python
# 示例嵌套列表
list_of_lists = [[1, 2], [3, 4], [1, 2], [5, 6], [3, 4]]

# 使用 set 去重
unique_list_of_lists = [list(t) for t in set(tuple(lst) for lst in list_of_lists)]
```
2. 利用set单纯遍历
```python
# 去重且保持顺序
seen = set()
unique_list_of_lists = []
for lst in list_of_lists:
    t = tuple(lst)
    if t not in seen:
        seen.add(t)
        unique_list_of_lists.append(lst)
```


