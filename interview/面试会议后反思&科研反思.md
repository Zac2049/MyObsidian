## 240131 数坤

- ==部分标注问题==中 将one hot 设为输入是否有必要（感觉没有
- dice loss 和 ce loss哪个更能解决小物体分割问题，dice loss集中于计算各个物体的交并情况，是各个比值的平均，ce loss其实是像素级的概率分布的偏差，本身对小物体不利。但我们既要dice loss也要ce loss因为dice loss虽然也能反映两个分布的差异情况，但会出现失之毫厘差之千里的情况，ce loss能弥补这点
- dice loss实际是拿概率值进行计算，而IoU loss是用sigmoid或者softmax后的离散值进行计算，本身有一些平滑的效果。（建议再详细看看这两者的差别
### CLIP驱动的分割问题

关于prompt的问题，prompt应该如何设计，不同的prompt会给出不同的pred。
>这是个复杂的问题，因余力不足，我的工作仅限于做简单的实验，展现不同prompt的影响

为什么有效？
我的观点，预训练的CLIP准确，提供标签间的语义信息。（问：标签之间的语义信息也可以分化标签来解决，为什么还需要这个）。（问：这些语义信息能多大提升模型的精度？答：实验体现）

text encoder和image encoder 是不是能同时frozen，结合那几篇论文想想

## 240220图灵量子

`watch -n 10 nvidia-smi` 每隔十秒动态周期刷新结果

### K means

K均值（K-Means）是一种常用的聚类算法，用于将数据集划分为K个不同的簇。以下是K均值的具体计算过程：

1. **初始化：** 随机选择K个数据点作为初始簇的中心（质心）。

2. **分配：** 对每个数据点，计算其与K个质心的距离，将其分配到距离最近的簇。

3. **更新：** 对每个簇，计算其所有成员的平均值，将该平均值作为新的质心。

4. **迭代：** 重复步骤2和步骤3，直到质心的变化小于某个阈值，或者达到预定的迭代次数。

下面以一个简单的例子来说明K均值的计算过程：

假设有以下数据集：

\[ X = \{(2, 3), (5, 3), (1, 1), (8, 5), (7, 2), (6, 4), (2, 9), (4, 8)\} \]

我们想要将这些点分为两个簇（K=2）。

**Step 1: 初始化**
随机选择两个数据点作为初始质心，比如选择 (2, 3) 和 (5, 3)。

**Step 2: 分配**
计算每个数据点到两个质心的距离，将其分配到最近的簇：

- 簇1: \((2, 3), (1, 1), (2, 9), (4, 8)\)
- 簇2: \((5, 3), (8, 5), (7, 2), (6, 4)\)

**Step 3: 更新**
计算每个簇的新质心：

- 新质心1: \((2.25, 5.25)\)
- 新质心2: \((6.5, 3.5)\)

**Step 4: 迭代**
重复进行步骤2和步骤3，直到质心的变化小于某个阈值，或者达到预定的迭代次数。

这样，经过几轮迭代，K均值算法会不断调整质心，最终将数据划分成两个簇。这是一个简化的例子，实际应用中可能需要多次运行算法，因为初始质心的选择可能影响最终的聚类结果。





### Adam的学习率调整策略，学习率是根据梯度调整的，具体根据其一阶矩和二阶矩

截止到我知识截断日期（2022年1月），以下是一些最新和最常用的优化器以及它们的原理。请注意，领域和研究的发展可能导致新的优化器的出现或一些方法的更新。

1. **Adam (Adaptive Moment Estimation):**
   - **原理：** Adam结合了梯度的一阶矩估计（梯度的指数移动平均）和二阶矩估计（梯度平方的指数移动平均）。它具有自适应学习率，使得对于不同参数的梯度可以使用不同的学习率。以及考虑==动量==，及过去的梯度对现在的梯度更新有一个势能，惯性
   - **公式：** \(m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t\), \(v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2\), \(\hat{\alpha}_t = \frac{\alpha \sqrt{1 - \beta_2^t}}{1 - \beta_1^t}\), \(w_{t+1} = w_t - \hat{\alpha}_t \cdot \frac{m_t}{\sqrt{v_t} + \epsilon}\).
   - **备注：** \(\beta_1\) 和 \(\beta_2\) 是衰减因子，\(\epsilon\) 是平滑项，\(\alpha\) 是初始学习率。

2. **SGD (Stochastic Gradient Descent):**
   - **原理：** SGD是最基本的优化算法，它在每一步使用整个训练集中的一个随机样本来更新模型参数。随机性有助于逃离局部最小值，但可能导致参数更新的不稳定。
   - **公式：** \(w_{t+1} = w_t - \eta \cdot g_t\), 其中 \(g_t\) 是在时间步 \(t\) 的梯度，\(\eta\) 是学习率。

3. **RMSprop (Root Mean Square Propagation):**
   - **原理：** RMSprop也是一种自适应学习率的算法，它使用梯度平方的==指数移动平均==来调整学习率。相比于Adam，RMSprop没有使用一阶矩估计。
   - **公式：** \(v_t = \beta \cdot v_{t-1} + (1 - \beta) \cdot g_t^2\), \(w_{t+1} = w_t - \frac{\eta}{\sqrt{v_t} + \epsilon} \cdot g_t\), 其中 \(\beta\) 是衰减因子，\(\epsilon\) 是平滑项。

1. **AdaGrad (Adaptive Gradient Algorithm):**
   - **原理：** AdaGrad是一种自适应学习率的算法，它根据每个参数的历史梯度平方和调整学习率，对于频繁出现的梯度较小的参数，学习率会较大。
   - **公式：** \(s_t = s_{t-1} + g_t^2\), \(w_{t+1} = w_t - \frac{\eta}{\sqrt{s_t} + \epsilon} \cdot g_t\), 其中 \(s_t\) 是历史梯度平方和。

是的，AdamW是对Adam优化算法的一种改进。它主要通过对==权重衰减（Weight Decay）的处理方式进行调整，以减少对模型权重的正则化影响==。

在Adam中，权重衰减通常是通过在梯度更新时简单地添加一个与==权重成比例的项==来实现的。但这种方法可能会导致对学习率的==过度惩罚==，尤其是在训练过程的早期阶段。

AdamW提出了一种更为合理的权重衰减方式，它将权重衰减的项添加到损失函数的计算中，而不是梯度更新的计算中。具体而言，对于一个具有权重衰减超参数 \(\lambda\) 的参数 \(w\)，AdamW的损失计算方式如下：

$$ L_{\text{AdamW}} = L_{\text{original}} + \frac{1}{2} \cdot \lambda \cdot \|w\|_2^2 $$

其中，\(L_{\text{original}}\) 是原始的损失函数。这样做的目的是让权重衰减的影响更显式地体现在损失函数中，而不是直接作用于梯度更新。

总的来说，AdamW相较于Adam的改进主要集中在对权重衰减的处理上，==旨在减小权重衰减对学习率的不必要惩罚==，从而更有效地优化模型。在一些实验中，AdamW在一些任务上的性能优于传统的Adam算法。


IoU计算，一般不需要计算前景后景，计算目标框和真实框

设计模式

有时候缺少驱动力

## 240221笔试

AI算法工程师是否存在某个题库

回溯问题，出了几个

KMP
"前缀"指除了最后一个字符以外，一个字符串的全部头部组合；"后缀"指除了第一个字符以外，一个字符串的全部尾部组合。

　"next值"就是"前缀"和"后缀"的最长的共有元素的长度。以"ABCDABD"为例，

－　"A"的前缀和后缀都为空集，共有元素的长度为0； 　　
－　"AB"的前缀为[A]，后缀为[B]，共有元素的长度为0； 　　
－　"ABC"的前缀为[A, AB]，后缀为[BC, C]，共有元素的长度0； 　　
－　"ABCD"的前缀为[A, AB, ABC]，后缀为[BCD, CD, D]，共有元素的长度为0； 　　
－　"ABCDA"的前缀为[A, AB, ABC, ABCD]，后缀为[BCDA, CDA, DA, A]，共有元素为"A"，长度为1； 　　
－　"ABCDAB"的前缀为[A, AB, ABC, ABCD, ABCDA]，后缀为[BCDAB, CDAB, DAB, AB, B]，共有元素为"AB"，长度为2； 　　－　"ABCDABD"的前缀为[A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为[BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为0